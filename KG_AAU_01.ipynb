{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09224dc9-5a3b-4bbf-b352-d6ec794235cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.11/site-packages (0.3.24)\n",
      "Requirement already satisfied: langchain-openai in /opt/conda/lib/python3.11/site-packages (0.3.14)\n",
      "Collecting langchain-google-genai\n",
      "  Downloading langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting langchain-groq\n",
      "  Downloading langchain_groq-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-experimental in /opt/conda/lib/python3.11/site-packages (0.3.4)\n",
      "Requirement already satisfied: langchain-neo4j in /opt/conda/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: neo4j in /opt/conda/lib/python3.11/site-packages (5.28.1)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.11/site-packages (2.11.3)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.11/site-packages (8.1.6)\n",
      "Requirement already satisfied: openai in /opt/conda/lib/python3.11/site-packages (1.76.0)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.11/site-packages (0.9.0)\n",
      "Requirement already satisfied: langchain-core in /opt/conda/lib/python3.11/site-packages (0.3.56)\n",
      "Requirement already satisfied: langchain-community in /opt/conda/lib/python3.11/site-packages (0.3.22)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/conda/lib/python3.11/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/conda/lib/python3.11/site-packages (from langchain) (0.3.37)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.0.26)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting google-ai-generativelanguage<0.7.0,>=0.6.16 (from langchain-google-genai)\n",
      "  Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
      "  Downloading groq-0.23.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: neo4j-graphrag<2.0.0,>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from langchain-neo4j) (1.6.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from neo4j) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/conda/lib/python3.11/site-packages (from pydantic) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.11/site-packages (from pydantic) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic) (0.4.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.14 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (3.0.14)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.11/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.11/site-packages (from langchain-community) (3.11.18)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.11/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/conda/lib/python3.11/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading google_auth-2.39.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /opt/conda/lib/python3.11/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.25.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: fsspec<2025.0.0,>=2024.9.0 in /opt/conda/lib/python3.11/site-packages (from neo4j-graphrag<2.0.0,>=1.5.0->langchain-neo4j) (2024.12.0)\n",
      "Requirement already satisfied: json-repair<0.40.0,>=0.39.1 in /opt/conda/lib/python3.11/site-packages (from neo4j-graphrag<2.0.0,>=1.5.0->langchain-neo4j) (0.39.1)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /opt/conda/lib/python3.11/site-packages (from neo4j-graphrag<2.0.0,>=1.5.0->langchain-neo4j) (5.4.0)\n",
      "Requirement already satisfied: types-pyyaml<7.0.0.0,>=6.0.12.20240917 in /opt/conda/lib/python3.11/site-packages (from neo4j-graphrag<2.0.0,>=1.5.0->langchain-neo4j) (6.0.12.20250402)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.62.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.60.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.60.1)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading langchain_google_genai-2.1.3-py3-none-any.whl (43 kB)\n",
      "Downloading langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading groq-0.23.1-py3-none-any.whl (127 kB)\n",
      "Downloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.39.0-py2.py3-none-any.whl (212 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: filetype, pyasn1, proto-plus, cachetools, rsa, pyasn1-modules, groq, google-auth, google-api-core, langchain-groq, google-ai-generativelanguage, langchain-google-genai\n",
      "Successfully installed cachetools-5.5.2 filetype-1.2.0 google-ai-generativelanguage-0.6.17 google-api-core-2.24.2 google-auth-2.39.0 groq-0.23.1 langchain-google-genai-2.1.3 langchain-groq-0.3.2 proto-plus-1.26.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-openai langchain-google-genai langchain-groq langchain-experimental langchain-neo4j neo4j pydantic python-dotenv ipywidgets openai tiktoken langchain-core langchain-community --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07175c71-d6ec-41af-af19-19c0f81cef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloque 1: Librerías importadas y logging configurado.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Script para procesar documentos de texto, extraer estructuras de grafo\n",
    "(nodos y relaciones) usando un LLM (OpenAI, Google, Groq, Ollama),\n",
    "y guardar esta estructura en archivos JSON detallados.\n",
    "\n",
    "Opcionalmente, puede cargar la estructura extraída en una base de datos Neo4j.\n",
    "\n",
    "Diseñado para ejecución modular en Jupyter Notebooks (abrir este .py o usar %run).\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# %% Bloque 1: Importaciones y Configuración Inicial\n",
    "# ============================================================\n",
    "# Nota: Si faltan librerías, ejecuta la siguiente línea en una celda separada UNA VEZ:\n",
    "# !pip install langchain langchain-openai langchain-google-genai langchain-groq langchain-experimental langchain-neo4j neo4j pydantic python-dotenv ipywidgets openai tiktoken langchain-core langchain-community --upgrade\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import webbrowser\n",
    "import json # Para manejar JSON\n",
    "from typing import List, Optional, Dict, Any\n",
    "from urllib.parse import urlparse # Para derivar URL del navegador\n",
    "import traceback # Para logs de errores detallados\n",
    "\n",
    "# Librerías de Terceros\n",
    "import dotenv\n",
    "from neo4j import GraphDatabase, Driver\n",
    "from pydantic import BaseModel, Field, ValidationError # Pydantic v2\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers.pydantic import PydanticOutputParser\n",
    "from langchain.output_parsers import OutputFixingParser # Para robustez extra\n",
    "# LLM Providers\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # Para Gemini API\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "# Graph Components\n",
    "from langchain_neo4j import Neo4jGraph # Aún necesario si se carga a Neo4j\n",
    "\n",
    "# Configuración del Logging\n",
    "log_format = '%(asctime)s - %(levelname)s - [%(name)s:%(lineno)d] - %(message)s'\n",
    "# Configurar para que también salga en la consola del notebook\n",
    "logging.basicConfig(level=logging.INFO, format=log_format, handlers=[\n",
    "    logging.StreamHandler() # Añade salida a consola/stderr\n",
    "    # Podrías añadir FileHandler si quieres guardar logs a archivo también\n",
    "    # logging.FileHandler(\"graph_processor.log\")\n",
    "])\n",
    "# Silenciar logs muy verbosos si es necesario\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n",
    "# Ajustar nivel de logs de Neo4j si es necesario (INFO, WARNING, ERROR)\n",
    "# logging.getLogger(\"neo4j\").setLevel(logging.WARNING)\n",
    "\n",
    "logger = logging.getLogger(__name__) # Logger específico para este script\n",
    "\n",
    "print(\"Bloque 1: Librerías importadas y logging configurado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a48ca39-fe94-4a64-9761-bf9fb88cd536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:48:04,016 - INFO - [__main__:5] - Bloque 2: Cargando variables de entorno y definiendo parámetros...\n",
      "2025-04-27 15:48:04,019 - INFO - [__main__:13] - Variables .env cargadas desde: /home/jovyan/work/KnowledgeGraph/KG-AAU/.env\n",
      "2025-04-27 15:48:04,021 - INFO - [__main__:104] - Directorio de salida para JSON asegurado: 'output_graphs'\n",
      "2025-04-27 15:48:04,023 - INFO - [__main__:146] - Bloque 2: Parámetros definidos y verificados.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Configuración Cargada y Verificada ---\n",
      "Neo4j URI: bolt://TIKA_Neo4j_02:7687\n",
      "Neo4j User: neo4j\n",
      "Neo4j Password Cargada: Sí\n",
      "Neo4j Database: default\n",
      "--------------------\n",
      "LLM Principal (Extracción JSON): groq\n",
      "  Modelo Principal Groq: llama3-8b-8192\n",
      "  OpenAI API Key Cargada: Sí\n",
      "  Google API Key Cargada: Sí\n",
      "  Groq API Key Cargada: Sí\n",
      "--------------------\n",
      "LLM para Chunking (JSON): Ollama (Siempre)\n",
      "  Modelo Chunking Ollama: gemma3:27b\n",
      "  URL Base Ollama (Chunking): http://TIKA_ollama:11434\n",
      "--------------------\n",
      "Archivos a procesar: datasets/Conciencia.md, datasets/Microsoft.md\n",
      "Directorio de Salida JSON: output_graphs\n",
      "Saltar Extracción JSON: False\n",
      "Imprimir Chunks: False\n",
      "Visualizar JSON Extraído: True\n",
      "Ejecutar Consultas Interactivas: True\n",
      "--------------------\n",
      "Cargar a Neo4j (Opcional): True\n",
      "  Neo4j Configurado: Sí\n",
      "  Borrar Grafo Neo4j antes de Cargar: True\n",
      "  Borrar Índices Neo4j antes de Cargar: True\n",
      "  Abrir Neo4j Browser (post-carga): True\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# %% Bloque 2: Carga de Variables de Entorno y Parámetros\n",
    "# ============================================================\n",
    "\n",
    "logger.info(\"Bloque 2: Cargando variables de entorno y definiendo parámetros...\")\n",
    "\n",
    "# --- Carga de Variables de Entorno ---\n",
    "# Busca .env en directorio actual y superiores\n",
    "# Contenido esperado: NEO4J_..., OPENAI_API_KEY, GOOGLE_API_KEY, GROQ_API_KEY, etc.\n",
    "env_path = dotenv.find_dotenv(usecwd=True) # Busca primero en el directorio actual\n",
    "if env_path:\n",
    "    loaded = dotenv.load_dotenv(dotenv_path=env_path, override=True)\n",
    "    logger.info(f\"Variables .env cargadas desde: {env_path}\" if loaded else f\".env encontrado ({env_path}) pero carga falló.\")\n",
    "else:\n",
    "    logger.warning(\".env no encontrado en directorio actual o superiores. Usando variables de entorno del sistema o valores por defecto.\")\n",
    "\n",
    "# --- Credenciales y Configuraciones ---\n",
    "# Asegúrate de que estas variables se carguen correctamente desde .env o el entorno\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\", \"neo4j\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "NEO4J_DATABASE: Optional[str] = os.getenv(\"NEO4J_DATABASE\", None) # None usa la DB por defecto ('neo4j')\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\") # Para Gemini API\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "\n",
    "# --- Parámetros de Configuración del Script ---\n",
    "# Lista de archivos de entrada a procesar (relativa al directorio donde se ejecuta el script/notebook)\n",
    "input_filepaths: List[str] = [\n",
    "    \"datasets/Conciencia.md\",\n",
    "    \"datasets/Microsoft.md\",\n",
    "    # Añade más archivos aquí\n",
    "]\n",
    "# Directorio para guardar los JSON (relativo al directorio de ejecución)\n",
    "# Asegúrate de que este directorio exista o se pueda crear.\n",
    "output_directory: str = \"output_graphs\"\n",
    "\n",
    "# --- Configuración LLM Principal (Extracción de Grafo JSON) ---\n",
    "llm_type: str = \"groq\" # Opciones: \"openai\", \"google\", \"groq\", \"ollama\"\n",
    "# Ajusta los modelos según el llm_type elegido y tu disponibilidad/preferencia\n",
    "openai_main_model_name: str = \"gpt-4o-mini\" # Modelo OpenAI\n",
    "google_main_model_name: str = \"gemini-1.5-flash-latest\" # Modelo Google Gemini\n",
    "groq_main_model_name: str = \"llama3-8b-8192\" # Modelo Groq (ej: llama3-70b-8192, mixtral-8x7b-32768)\n",
    "ollama_main_model_name: str = \"gemma3:27b\"\n",
    "\n",
    "# Modelos disponibles en ollama en mi configuración\n",
    "#NAME                       ID              SIZE      MODIFIED     \n",
    "#llama3.2-vision:latest     085a1fdae525    7.9 GB    47 hours ago    \n",
    "#deepseek-coder:1.3b        3ddd2d3fc8d2    776 MB    4 days ago      \n",
    "#codegemma:2b               926331004170    1.6 GB    4 days ago      \n",
    "#qwen:4b                    d53d04290064    2.3 GB    4 days ago      \n",
    "#llava:latest               8dd30f6b0cb1    4.7 GB    5 days ago      \n",
    "#llama3.1:8b                46e0c10c039e    4.9 GB    2 weeks ago     \n",
    "#llama3.2:3b                a80c4f17acd5    2.0 GB    2 weeks ago     \n",
    "#nomic-embed-text:latest    0a109f422b47    274 MB    3 weeks ago     \n",
    "#gemma3:27b                 a418f5838eaf    17 GB     3 weeks ago     \n",
    "#gemma3:latest              a2af6cc3eb7f    3.3 GB    3 weeks ago     \n",
    "#qwq:latest                 009cb3f08d74    19 GB     3 weeks ago     \n",
    "\n",
    "# \" # Modelo principal si llm_type es \"ollama\"\n",
    "\n",
    "# --- Configuración LLM para Chunking (SIEMPRE Ollama + JSON Mode) ---\n",
    "# Se usa Ollama para chunking por la fiabilidad de `format=\"json\"`.\n",
    "# Asegúrate de que Ollama esté corriendo en OLLAMA_BASE_URL y tenga este modelo.\n",
    "ollama_chunking_model_name: str = \"gemma3:27b\" # Modelo Ollama específico para chunking JSON\n",
    "# Ejecuta `ollama pull gemma:7b` en tu servidor Ollama si no lo tienes.\n",
    "\n",
    "# --- Control del Flujo ---\n",
    "# Define qué partes del script se ejecutarán\n",
    "skip_extraction: bool = False      # True para saltar lectura, chunking y extracción JSON\n",
    "print_chunks: bool = False         # True para imprimir chunks intermedios de Ollama\n",
    "visualize_json: bool = True        # True para imprimir el JSON extraído final en la salida\n",
    "load_into_neo4j: bool = True       # True para intentar cargar el JSON extraído a Neo4j\n",
    "# --- >>> LÍNEA AÑADIDA/CORREGIDA <<< ---\n",
    "run_interactive_query: bool = True # True para iniciar el bucle de consultas interactivas (si load_into_neo4j=True)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- Opciones Neo4j (SOLO si load_into_neo4j = True) ---\n",
    "# Estas opciones solo tienen efecto si load_into_neo4j es True\n",
    "clear_graph_before_load: bool = True # Borrar grafo Neo4j ANTES de cargar los datos\n",
    "delete_indexes_before_load: bool = True # Borrar índices Neo4j definidos por usuario ANTES de cargar\n",
    "\n",
    "# --- Opciones de Visualización Externa ---\n",
    "# Esta opción solo tiene efecto si load_into_neo4j es True\n",
    "show_neo4j_browser: bool = True    # Intentar abrir Neo4j Browser después de la carga\n",
    "\n",
    "# --- Verificación de Credenciales Críticas ---\n",
    "# Comprueba si hay contraseña para Neo4j, necesaria si se carga/consulta\n",
    "neo4j_configured = bool(NEO4J_PASSWORD)\n",
    "if load_into_neo4j and not neo4j_configured:\n",
    "    logger.warning(\"load_into_neo4j=True pero falta NEO4J_PASSWORD. La carga a Neo4j fallará.\")\n",
    "# Comprueba las API keys según el LLM principal seleccionado\n",
    "if llm_type == \"openai\" and not OPENAI_API_KEY: logger.warning(\"llm_type='openai' pero OPENAI_API_KEY falta.\")\n",
    "if llm_type == \"google\" and not GOOGLE_API_KEY: logger.warning(\"llm_type='google' pero GOOGLE_API_KEY falta.\")\n",
    "if llm_type == \"groq\" and not GROQ_API_KEY: logger.warning(\"llm_type='groq' pero GROQ_API_KEY falta.\")\n",
    "# No se necesita API key para Ollama local si es el principal\n",
    "\n",
    "# Crear directorio de salida si no existe y no se salta la extracción\n",
    "if not skip_extraction:\n",
    "    try:\n",
    "        # Crear el directorio si no existe. parents=True crea directorios intermedios si son necesarios.\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        logger.info(f\"Directorio de salida para JSON asegurado: '{output_directory}'\")\n",
    "    except OSError as e:\n",
    "        logger.error(f\"No se pudo crear el directorio de salida '{output_directory}': {e}. La escritura de JSON fallará.\")\n",
    "        # Considerar detener el script aquí si la salida es esencial\n",
    "        # raise OSError(f\"Fallo al crear directorio de salida: {e}\")\n",
    "\n",
    "# --- Imprimir Configuración Final ---\n",
    "# Muestra todos los parámetros tal como se usarán\n",
    "print(\"\\n--- Configuración Cargada y Verificada ---\")\n",
    "print(f\"Neo4j URI: {NEO4J_URI}\")\n",
    "print(f\"Neo4j User: {NEO4J_USERNAME}\")\n",
    "print(f\"Neo4j Password Cargada: {'Sí' if NEO4J_PASSWORD else 'No'}\")\n",
    "print(f\"Neo4j Database: {NEO4J_DATABASE if NEO4J_DATABASE else 'default'}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"LLM Principal (Extracción JSON): {llm_type}\")\n",
    "if llm_type == \"openai\": print(f\"  Modelo Principal OpenAI: {openai_main_model_name}\")\n",
    "elif llm_type == \"google\": print(f\"  Modelo Principal Google: {google_main_model_name}\")\n",
    "elif llm_type == \"groq\": print(f\"  Modelo Principal Groq: {groq_main_model_name}\")\n",
    "elif llm_type == \"ollama\": print(f\"  Modelo Principal Ollama: {ollama_main_model_name}\")\n",
    "# Indicar qué API keys se encontraron\n",
    "if OPENAI_API_KEY: print(f\"  OpenAI API Key Cargada: Sí\")\n",
    "if GOOGLE_API_KEY: print(f\"  Google API Key Cargada: Sí\")\n",
    "if GROQ_API_KEY: print(f\"  Groq API Key Cargada: Sí\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"LLM para Chunking (JSON): Ollama (Siempre)\")\n",
    "print(f\"  Modelo Chunking Ollama: {ollama_chunking_model_name}\")\n",
    "print(f\"  URL Base Ollama (Chunking): {OLLAMA_BASE_URL}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Archivos a procesar: {', '.join(input_filepaths)}\")\n",
    "print(f\"Directorio de Salida JSON: {output_directory}\")\n",
    "print(f\"Saltar Extracción JSON: {skip_extraction}\")\n",
    "print(f\"Imprimir Chunks: {print_chunks}\")\n",
    "print(f\"Visualizar JSON Extraído: {visualize_json}\")\n",
    "print(f\"Ejecutar Consultas Interactivas: {run_interactive_query}\") # <-- Mostrar valor\n",
    "print(\"-\" * 20)\n",
    "print(f\"Cargar a Neo4j (Opcional): {load_into_neo4j}\")\n",
    "if load_into_neo4j:\n",
    "    print(f\"  Neo4j Configurado: {'Sí' if neo4j_configured else 'NO (Falta Password!)'}\")\n",
    "    print(f\"  Borrar Grafo Neo4j antes de Cargar: {clear_graph_before_load}\")\n",
    "    print(f\"  Borrar Índices Neo4j antes de Cargar: {delete_indexes_before_load}\")\n",
    "    print(f\"  Abrir Neo4j Browser (post-carga): {show_neo4j_browser}\") # <-- Mostrar valor\n",
    "print(\"-----------------------------\")\n",
    "logger.info(\"Bloque 2: Parámetros definidos y verificados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d1863bd-5469-4d3a-9c5e-20d862afe732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:48:07,944 - INFO - [__main__:4] - Bloque 3: Definiendo modelos Pydantic para estructura de datos...\n",
      "2025-04-27 15:48:07,949 - INFO - [__main__:36] - Bloque 3: Modelos Pydantic completados.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos Pydantic definidos: Chunk, Chunks, Node, Relationship, ExtractedGraph.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# %% Bloque 3: Definición de Modelos Pydantic\n",
    "# ============================================================\n",
    "logger.info(\"Bloque 3: Definiendo modelos Pydantic para estructura de datos...\")\n",
    "# Usamos Pydantic V2 (importado como pydantic sin .v1)\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    \"\"\"Representa un trozo de texto semánticamente coherente.\"\"\"\n",
    "    chunk_id: int = Field(..., description=\"ID numérico secuencial único del chunk (empezando en 1).\")\n",
    "    text: str = Field(..., description=\"Contenido textual del chunk, procesado semánticamente.\")\n",
    "\n",
    "class Chunks(BaseModel):\n",
    "    \"\"\"Esquema JSON esperado como salida del LLM de chunking (Ollama). Contiene lista de Chunks.\"\"\"\n",
    "    chunks: List[Chunk] = Field(..., description=\"Lista ordenada de los chunks generados del texto original.\")\n",
    "\n",
    "# --- Modelos para Extracción de Grafo (Salida del LLM Principal) ---\n",
    "class Node(BaseModel):\n",
    "    \"\"\"Representa un nodo en el grafo de conocimiento.\"\"\"\n",
    "    id: str = Field(..., description=\"Identificador único y canónico del nodo (e.g., nombre normalizado).\")\n",
    "    label: str = Field(..., description=\"Etiqueta principal del nodo (e.g., Person, Organization, Concept).\")\n",
    "    properties: Dict[str, Any] = Field(default_factory=dict, description=\"Propiedades adicionales (clave-valor) extraídas del texto.\")\n",
    "\n",
    "class Relationship(BaseModel):\n",
    "    \"\"\"Representa una relación dirigida entre dos nodos identificados por su ID.\"\"\"\n",
    "    source: str = Field(..., description=\"ID del nodo origen.\")\n",
    "    target: str = Field(..., description=\"ID del nodo destino.\")\n",
    "    type: str = Field(..., description=\"Tipo de la relación en MAYUSCULAS_CON_GUIONES (e.g., WORKS_AT, LOCATED_IN).\")\n",
    "    properties: Dict[str, Any] = Field(default_factory=dict, description=\"Propiedades de la relación (clave-valor).\")\n",
    "\n",
    "class ExtractedGraph(BaseModel):\n",
    "    \"\"\"Representa la estructura completa del grafo extraída de un texto o chunk.\"\"\"\n",
    "    nodes: List[Node] = Field(default_factory=list, description=\"Lista de nodos únicos identificados.\")\n",
    "    relationships: List[Relationship] = Field(default_factory=list, description=\"Lista de relaciones identificadas entre los nodos.\")\n",
    "\n",
    "print(\"Modelos Pydantic definidos: Chunk, Chunks, Node, Relationship, ExtractedGraph.\")\n",
    "logger.info(\"Bloque 3: Modelos Pydantic completados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "491f0a3c-d96d-4877-81b4-c76bdc264c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:48:09,827 - INFO - [__main__:4] - Bloque 4: Inicializando el LLM principal según configuración...\n",
      "2025-04-27 15:48:09,828 - INFO - [__main__:21] - Intentando inicializar LLM principal: groq\n",
      "2025-04-27 15:48:09,990 - INFO - [__main__:52] - LLM principal inicializado: groq (Modelo: llama3-8b-8192)\n",
      "2025-04-27 15:48:09,991 - INFO - [__main__:92] - Bloque 4: Inicialización LLM principal completada (o fallida).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM principal (groq) inicializado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# %% Bloque 4: Inicialización del LLM Principal\n",
    "# ============================================================\n",
    "logger.info(\"Bloque 4: Inicializando el LLM principal según configuración...\")\n",
    "\n",
    "def get_main_llm(\n",
    "    llm_provider: str,\n",
    "    # OpenAI\n",
    "    openai_key: Optional[str], openai_model: str,\n",
    "    # Google\n",
    "    google_key: Optional[str], google_model: str,\n",
    "    # Groq\n",
    "    groq_key: Optional[str], groq_model: str,\n",
    "    # Ollama\n",
    "    ollama_url: str, ollama_model: str\n",
    ") -> Optional[BaseChatModel]:\n",
    "    \"\"\"\n",
    "    Inicializa y devuelve la instancia del LLM principal (para extracción JSON).\n",
    "    Maneja OpenAI, Google Gemini, Groq, y Ollama.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Intentando inicializar LLM principal: {llm_provider}\")\n",
    "    llm_instance: Optional[BaseChatModel] = None\n",
    "    try:\n",
    "        if llm_provider == \"openai\":\n",
    "            if not openai_key: raise ValueError(\"OPENAI_API_KEY es requerida.\")\n",
    "            llm_instance = ChatOpenAI(model=openai_model, temperature=0, api_key=openai_key)\n",
    "        elif llm_provider == \"google\":\n",
    "            if not google_key: raise ValueError(\"GOOGLE_API_KEY es requerida.\")\n",
    "            llm_instance = ChatGoogleGenerativeAI(model=google_model, google_api_key=google_key, temperature=0, convert_system_message_to_human=True)\n",
    "        elif llm_provider == \"groq\":\n",
    "            if not groq_key: raise ValueError(\"GROQ_API_KEY es requerida.\")\n",
    "            llm_instance = ChatGroq(groq_api_key=groq_key, model_name=groq_model, temperature=0)\n",
    "        elif llm_provider == \"ollama\":\n",
    "            if not ollama_model: raise ValueError(\"ollama_main_model_name es requerido.\")\n",
    "            # Asegurarse que la URL base sea correcta\n",
    "            if not ollama_url or not urlparse(ollama_url).scheme:\n",
    "                 raise ValueError(f\"URL base de Ollama inválida o faltante: '{ollama_url}'\")\n",
    "            llm_instance = ChatOllama(model=ollama_model, base_url=ollama_url, temperature=0)\n",
    "        else:\n",
    "            logger.error(f\"Tipo de LLM principal no soportado: '{llm_provider}'\")\n",
    "            return None\n",
    "\n",
    "        # Determinar nombre del modelo para log\n",
    "        model_name_for_log = 'N/A'\n",
    "        if hasattr(llm_instance, 'model_name'):\n",
    "            model_name_for_log = llm_instance.model_name\n",
    "        elif hasattr(llm_instance, 'model'): # Ollama usa 'model'\n",
    "             model_name_for_log = llm_instance.model\n",
    "        elif llm_provider == \"google\": # Google puede no tenerlo directamente\n",
    "             model_name_for_log = google_model # Usar el nombre pasado\n",
    "\n",
    "        logger.info(f\"LLM principal inicializado: {llm_provider} (Modelo: {model_name_for_log})\")\n",
    "\n",
    "\n",
    "        # Opcional: Test rápido de conectividad\n",
    "        # logger.debug(f\"Realizando test rápido LLM {llm_provider}...\")\n",
    "        # llm_instance.invoke(\"Confirma status.\")\n",
    "        # logger.info(f\"Test rápido LLM {llm_provider} exitoso.\")\n",
    "\n",
    "        return llm_instance\n",
    "\n",
    "    except ImportError as e:\n",
    "         logger.error(f\"Fallo import librería para {llm_provider}: {e}. ¿Instalada?\", exc_info=False) # No mostrar traceback completo de import\n",
    "         print(f\"ERROR: Librería para {llm_provider} no encontrada. Instálala (ver Bloque 1).\")\n",
    "         return None\n",
    "    except ValueError as e: # Captura errores de validación (ej. falta de clave/URL)\n",
    "         logger.error(f\"Error configuración {llm_provider}: {e}\")\n",
    "         print(f\"ERROR: Configuración para {llm_provider} inválida: {e}\")\n",
    "         return None\n",
    "    except Exception as e: # Otros errores (conexión, API inválida, modelo no existe)\n",
    "        logger.error(f\"Fallo inesperado inicializando {llm_provider}: {e}\", exc_info=True) # Mostrar traceback aquí sí\n",
    "        print(f\"ERROR: Fallo al inicializar {llm_provider}. Verifica conexión, API keys, nombre del modelo y logs.\")\n",
    "        return None\n",
    "\n",
    "# --- Inicializar el LLM principal ---\n",
    "llm: Optional[BaseChatModel] = get_main_llm(\n",
    "    llm_provider=llm_type,\n",
    "    openai_key=OPENAI_API_KEY, openai_model=openai_main_model_name,\n",
    "    google_key=GOOGLE_API_KEY, google_model=google_main_model_name,\n",
    "    groq_key=GROQ_API_KEY, groq_model=groq_main_model_name,\n",
    "    ollama_url=OLLAMA_BASE_URL, ollama_model=ollama_main_model_name\n",
    ")\n",
    "\n",
    "if llm:\n",
    "    print(f\"LLM principal ({llm_type}) inicializado correctamente.\")\n",
    "else:\n",
    "    print(f\"¡FALLO al inicializar el LLM principal ({llm_type})! La extracción JSON no funcionará.\")\n",
    "    # Detener si el LLM es esencial para el flujo deseado\n",
    "    if not skip_extraction:\n",
    "         raise RuntimeError(f\"Fallo crítico: No se pudo inicializar el LLM principal '{llm_type}'.\")\n",
    "\n",
    "logger.info(\"Bloque 4: Inicialización LLM principal completada (o fallida).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de4e5642-c265-41dc-87ec-2e130cf2051b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:48:11,644 - INFO - [__main__:4] - Bloque 5: Definiendo funciones utilitarias para Neo4j (usadas si load_into_neo4j=True)...\n",
      "2025-04-27 15:48:11,647 - INFO - [__main__:310] - Bloque 5: Funciones utilitarias de Neo4j completadas.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones utilitarias de Neo4j definidas.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# %% Bloque 5: Funciones Utilitarias para Neo4j (Opcional)\n",
    "# ============================================================\n",
    "logger.info(\"Bloque 5: Definiendo funciones utilitarias para Neo4j (usadas si load_into_neo4j=True)...\")\n",
    "\n",
    "# Asegurar importaciones necesarias aquí también\n",
    "from neo4j import GraphDatabase, Driver\n",
    "from typing import Optional\n",
    "import logging\n",
    "import time\n",
    "import webbrowser\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Usar el logger principal configurado\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_neo4j_driver(uri: str, user: str, passw: str) -> Optional[Driver]:\n",
    "    \"\"\"Establece y verifica conexión directa con Neo4j.\"\"\"\n",
    "    if not passw:\n",
    "        logger.error(\"Se requiere contraseña de Neo4j para crear el driver.\")\n",
    "        print(\"ERROR: Falta contraseña Neo4j.\")\n",
    "        return None\n",
    "    try:\n",
    "        # Timeout de conexión y adquisición más generosos\n",
    "        driver = GraphDatabase.driver(uri, auth=(user, passw),\n",
    "                                      connection_timeout=15.0, # segundos\n",
    "                                      max_connection_lifetime=3600) # 1 hora\n",
    "        # Verificar conectividad y obtener info básica del servidor\n",
    "        driver.verify_connectivity()\n",
    "        server_info = driver.get_server_info()\n",
    "        # --- CORRECCIÓN: Log simplificado sin .version ---\n",
    "        # Acceder a atributos que sí existen en ServerInfo\n",
    "        logger.info(f\"Conexión directa con Neo4j en {uri} verificada (Server address: {server_info.address}, Protocol: {server_info.protocol_version}).\")\n",
    "        # También podrías loguear server_info.agent si es útil\n",
    "        # logger.info(f\"Server Agent: {server_info.agent}\")\n",
    "        # ------------------------------------------------\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fallo al conectar driver Neo4j en {uri}: {e}\", exc_info=True)\n",
    "        print(f\"ERROR: Fallo al conectar con Neo4j en {uri}. Verifica URI, credenciales y estado del servidor.\")\n",
    "        return None\n",
    "\n",
    "def reset_graph_data(driver: Driver, db_name: Optional[str] = None) -> bool:\n",
    "    \"\"\"Elimina TODOS los nodos y relaciones. Requiere doble confirmación.\"\"\"\n",
    "    effective_db = db_name if db_name else \"neo4j\" # Neo4j >= 4.0 default es 'neo4j'\n",
    "    # Usar input() solo si se ejecuta interactivamente\n",
    "    try:\n",
    "        print(f\"\\nADVERTENCIA MUY SERIA:\")\n",
    "        print(f\"Estás a punto de BORRAR **TODOS** los nodos y relaciones de la base de datos '{effective_db}'.\")\n",
    "        print(f\"Esta acción es IRREVERSIBLE.\")\n",
    "        confirm1 = input(f\"Escribe 'SI QUIERO BORRAR TODO' para continuar con la segunda confirmación: \")\n",
    "        if confirm1 != \"SI QUIERO BORRAR TODO\":\n",
    "             logger.warning(f\"Primera confirmación para borrado cancelada por el usuario para BD '{effective_db}'.\")\n",
    "             print(\"Borrado cancelado (Paso 1).\")\n",
    "             return False\n",
    "\n",
    "        print(f\"\\nSEGUNDA CONFIRMACIÓN (IRREVERSIBLE):\")\n",
    "        confirm2 = input(f\"Escribe 'BORRAR TODO NEO4J AHORA' para proceder con el borrado de '{effective_db}': \")\n",
    "        if confirm2 != \"BORRAR TODO NEO4J AHORA\":\n",
    "            logger.warning(f\"Segunda confirmación para borrado cancelada por el usuario para BD '{effective_db}'.\")\n",
    "            print(\"Borrado cancelado (Paso 2).\")\n",
    "            return False\n",
    "\n",
    "    except EOFError: # Manejar si no se ejecuta en un TTY interactivo\n",
    "         logger.error(\"No se pudo obtener confirmación interactiva para reset_graph_data. Abortando borrado.\")\n",
    "         print(\"ERROR: No se puede confirmar el borrado en un entorno no interactivo. Operación cancelada.\")\n",
    "         return False\n",
    "\n",
    "    logger.warning(f\"CONFIRMACIÓN DOBLE RECIBIDA. PROCEDIENDO CON BORRADO COMPLETO en BD '{effective_db}'...\")\n",
    "    try:\n",
    "        with driver.session(database=db_name) as session:\n",
    "            start_time = time.time()\n",
    "            logger.info(\"Ejecutando 'MATCH (n) DETACH DELETE n'...\")\n",
    "            # Ejecutar con timeout por si la base de datos es muy grande\n",
    "            result = session.run(\"MATCH (n) DETACH DELETE n\", timeout=300.0) # 5 minutos timeout\n",
    "            summary = result.consume() # Consumir para obtener estadísticas\n",
    "            duration = time.time() - start_time\n",
    "            # Acceder a los contadores del summary\n",
    "            nodes_deleted = summary.counters.nodes_deleted\n",
    "            rels_deleted = summary.counters.relationships_deleted\n",
    "            logger.info(f\"Datos del grafo reseteados en BD '{effective_db}' en {duration:.2f}s. Nodos borrados: {nodes_deleted}, Relaciones borradas: {rels_deleted}.\")\n",
    "            print(f\"Datos del grafo en '{effective_db}' borrados.\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al resetear datos en BD '{effective_db}': {e}\", exc_info=True)\n",
    "        print(f\"Error al borrar datos en '{effective_db}'. Ver logs.\")\n",
    "        return False\n",
    "\n",
    "def retrieve_graph_summary(driver: Driver, db_name: Optional[str] = None) -> str:\n",
    "    \"\"\"Recupera un resumen del contenido del grafo (conteos, labels, reltypes).\"\"\"\n",
    "    db_log_name = db_name if db_name else 'default'\n",
    "    summary = f\"Resumen del Grafo (Base de Datos: {db_log_name}):\\n\"\n",
    "    try:\n",
    "        with driver.session(database=db_name) as session:\n",
    "            # Intentar con APOC primero\n",
    "            apoc_summary = \"\"\n",
    "            try:\n",
    "                # Usar single() y verificar si devuelve None o un registro\n",
    "                stats_record = session.run(\"CALL apoc.meta.stats() YIELD nodeCount, relCount, labels, relTypes RETURN *\").single()\n",
    "                if stats_record: # Verificar que no sea None\n",
    "                    stats = stats_record.data() # Convertir a diccionario\n",
    "                    apoc_summary += f\"- Nodos Totales (APOC): {stats.get('nodeCount', 0)}\\n\" # Usar .get con default\n",
    "                    apoc_summary += f\"- Relaciones Totales (APOC): {stats.get('relCount', 0)}\\n\"\n",
    "                    # Filtrar tipos de relaciones internas de APOC\n",
    "                    rel_types_map = stats.get('relTypes', {})\n",
    "                    rel_types_filtered = {k: v for k, v in rel_types_map.items() if not k.startswith('_')}\n",
    "                    labels_map = stats.get('labels', {})\n",
    "                    apoc_summary += f\"- Tipos de Nodos (Labels): {', '.join(sorted(labels_map.keys())) if labels_map else 'Ninguno'}\\n\"\n",
    "                    apoc_summary += f\"- Tipos de Relaciones: {', '.join(sorted(rel_types_filtered.keys())) if rel_types_filtered else 'Ninguna'}\\n\"\n",
    "                    return apoc_summary.strip() # Devolver si APOC tuvo éxito\n",
    "                else:\n",
    "                    logger.debug(\"apoc.meta.stats() no devolvió resultados (single() fue None).\")\n",
    "            except Exception as apoc_e:\n",
    "                logger.debug(f\"apoc.meta.stats() falló ({type(apoc_e).__name__}), usando conteos manuales.\")\n",
    "\n",
    "            # Conteos manuales como fallback\n",
    "            manual_summary = \"\"\n",
    "            node_count_res = session.run(\"MATCH (n) RETURN count(n) AS count\").single()\n",
    "            node_count = node_count_res['count'] if node_count_res else 0\n",
    "            manual_summary += f\"- Nodos Totales (Manual): {node_count}\\n\"\n",
    "            if node_count == 0: return manual_summary + \"- Grafo vacío.\\n\"\n",
    "\n",
    "            rel_count_res = session.run(\"MATCH ()-[r]->() RETURN count(r) AS count\").single()\n",
    "            rel_count = rel_count_res['count'] if rel_count_res else 0\n",
    "            manual_summary += f\"- Relaciones Totales (Manual): {rel_count}\\n\"\n",
    "\n",
    "            labels_res = session.run(\"CALL db.labels() YIELD label RETURN collect(label) as labels\").single()\n",
    "            labels = sorted(labels_res['labels']) if labels_res and labels_res.get('labels') else []\n",
    "            manual_summary += f\"- Tipos de Nodos (Labels): {', '.join(labels) if labels else 'Ninguno'}\\n\"\n",
    "\n",
    "            rel_types_res = session.run(\"CALL db.relationshipTypes() YIELD relationshipType RETURN collect(relationshipType) as types\").single()\n",
    "            rel_types = sorted(rel_types_res['types']) if rel_types_res and rel_types_res.get('types') else []\n",
    "            # Filtrar tipos internos (puede ser redundante si APOC falló, pero seguro)\n",
    "            rel_types = [rt for rt in rel_types if not rt.startswith(\"_\")]\n",
    "            manual_summary += f\"- Tipos de Relaciones: {', '.join(rel_types) if rel_types else 'Ninguna'}\\n\"\n",
    "            return manual_summary.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error recuperando resumen del grafo BD '{db_log_name}': {e}\", exc_info=True)\n",
    "        return summary + f\"Error recuperando resumen: {e}\"\n",
    "\n",
    "def print_indexes(driver: Driver, db_name: Optional[str] = None):\n",
    "    \"\"\"Imprime los índices existentes en la base de datos de forma detallada.\"\"\"\n",
    "    effective_db = db_name if db_name else \"default\"\n",
    "    logger.info(f\"Consultando índices en BD '{effective_db}'...\")\n",
    "    print(f\"\\n--- Índices en Base de Datos: {effective_db} ---\")\n",
    "    try:\n",
    "        with driver.session(database=db_name) as session:\n",
    "            result = session.run(\"SHOW INDEXES\")\n",
    "            indexes = [record.data() for record in result] # Convertir a lista\n",
    "            if indexes:\n",
    "                indexes = sorted(indexes, key=lambda x: x.get('name', '')) # Ordenar por nombre\n",
    "                print(f\"Se encontraron {len(indexes)} índices:\")\n",
    "                for idx in indexes:\n",
    "                    name = idx.get('name', 'N/A')\n",
    "                    idx_type = idx.get('type', 'N/A')\n",
    "                    entity_type = idx.get('entityType', 'N/A')\n",
    "                    labels = idx.get('labelsOrTypes', []) # Default a lista vacía\n",
    "                    props = idx.get('properties', [])   # Default a lista vacía\n",
    "                    state = idx.get('state', 'N/A')\n",
    "                    # Usar join solo si la lista no está vacía/None\n",
    "                    labels_str = ', '.join(labels) if labels else 'N/A'\n",
    "                    props_str = ', '.join(props) if props else 'N/A'\n",
    "                    print(f\"- Nombre: {name} | Estado: {state} | TipoÍndice: {idx_type}\")\n",
    "                    print(f\"    TipoEntidad: {entity_type} | Labels/Tipos: {labels_str} | Propiedades: {props_str}\")\n",
    "                logger.info(f\"Se encontraron {len(indexes)} índices en BD '{effective_db}'.\")\n",
    "            else:\n",
    "                print(\"No se encontraron índices definidos.\")\n",
    "                logger.info(f\"No se encontraron índices en BD '{effective_db}'.\")\n",
    "    except Exception as e:\n",
    "        # Manejo de fallback para versiones antiguas (igual que antes)\n",
    "        if \"Unknown command `SHOW`\" in str(e) or \"Invalid input 'SHOW'\" in str(e):\n",
    "             print(\"Comando 'SHOW INDEXES' no soportado. Usando CALL db.indexes()...\")\n",
    "             try:\n",
    "                 with driver.session(database=db_name) as session_old:\n",
    "                     result_old = session_old.run(\"CALL db.indexes()\")\n",
    "                     indexes_old = [record_old.data() for record_old in result_old]\n",
    "                     if indexes_old:\n",
    "                         print(f\"Se encontraron {len(indexes_old)} índices (formato antiguo):\")\n",
    "                         # Formato puede variar, imprimir raw\n",
    "                         for idx_old in indexes_old: print(f\"  - {idx_old}\")\n",
    "                     else: print(\"No se encontraron índices (método alternativo).\")\n",
    "             except Exception as e_old: print(f\"Error CALL db.indexes(): {e_old}\")\n",
    "        else:\n",
    "            print(f\"Error al obtener índices: {e}\")\n",
    "            logger.error(f\"Error obteniendo índices BD '{effective_db}': {e}\", exc_info=True)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "def delete_all_user_indexes(driver: Driver, db_name: Optional[str] = None) -> bool:\n",
    "    \"\"\"Elimina TODOS los índices definidos por el usuario. Requiere confirmación.\"\"\"\n",
    "    effective_db = db_name if db_name else \"default\"\n",
    "    logger.warning(f\"Intentando eliminar índices de usuario en BD '{effective_db}'...\")\n",
    "    try:\n",
    "        with driver.session(database=db_name) as session:\n",
    "            # --- CONSULTA CYPHER CORREGIDA ---\n",
    "            # Usar YIELD para obtener las columnas y luego filtrar con WHERE y RETURN\n",
    "            cypher_get_indexes = \"\"\"\n",
    "            SHOW INDEXES\n",
    "            YIELD name, type // Especificar las columnas que necesitas\n",
    "            WHERE type <> 'LOOKUP' // Filtrar por tipo\n",
    "              AND NOT name STARTS WITH 'vector_' // Exclusiones por nombre\n",
    "              AND NOT name STARTS WITH 'graphrag_'\n",
    "              AND NOT name CONTAINS '_vector_'\n",
    "              AND NOT name IN $system_index_names // Excluir índices de sistema\n",
    "            RETURN name AS name_to_drop // Retornar solo el nombre para borrar\n",
    "            \"\"\"\n",
    "            # ---------------------------------------\n",
    "            # Nombres comunes de índices de sistema/constraint a excluir\n",
    "            params = {\"system_index_names\": [\"nodes_id_unique\", \"rels_id_unique\", \"node_constraint\", \"relationship_constraint\", \"token_lookup_nodes\", \"token_lookup_relationships\"]}\n",
    "\n",
    "            index_names_to_delete = []\n",
    "            try:\n",
    "                 result = session.run(cypher_get_indexes, parameters=params)\n",
    "                 index_names_to_delete = [record[\"name_to_drop\"] for record in result]\n",
    "                 logger.info(f\"Se encontraron {len(index_names_to_delete)} índices de usuario para posible borrado.\")\n",
    "            except Exception as show_e:\n",
    "                 # Manejo de fallback si SHOW INDEXES no funciona (igual que antes)\n",
    "                 if \"Unknown command `SHOW`\" in str(show_e) or \"Invalid input 'SHOW'\" in str(show_e):\n",
    "                     logger.warning(\"SHOW INDEXES falló/no soportado, intentando con db.indexes()...\")\n",
    "                     try:\n",
    "                         # Este fallback es menos preciso para identificar índices de sistema\n",
    "                         result_old = session.run(\"CALL db.indexes() YIELD name, type WHERE type <> 'LOOKUP' RETURN name\")\n",
    "                         index_names_to_delete = [r[\"name\"] for r in result_old if not r[\"name\"].startswith(\"constraint\") and not r[\"name\"].startswith(\"token\") and not r[\"name\"].startswith(\"vector\")]\n",
    "                         logger.info(f\"Fallback db.indexes() encontró {len(index_names_to_delete)} candidatos.\")\n",
    "                     except Exception as dbidx_e:\n",
    "                         logger.error(f\"CALL db.indexes() también falló: {dbidx_e}. No se pueden determinar índices a borrar.\")\n",
    "                         return False # Fallo crítico\n",
    "                 else:\n",
    "                     logger.error(f\"Error ejecutando SHOW INDEXES: {show_e}\", exc_info=True)\n",
    "                     print(\"Error listando índices, el borrado de índices fallará.\")\n",
    "                     return False # Indicar fallo\n",
    "\n",
    "            # --- Lógica de confirmación y borrado ---\n",
    "            if not index_names_to_delete:\n",
    "                logger.info(f\"No se encontraron índices de usuario para eliminar en BD '{effective_db}'.\")\n",
    "                print(f\"No hay índices de usuario para borrar en '{effective_db}'.\")\n",
    "                return True\n",
    "\n",
    "            print(f\"\\nSe borrarán los siguientes {len(index_names_to_delete)} índices de usuario en '{effective_db}':\")\n",
    "            for name in index_names_to_delete: print(f\"  - {name}\")\n",
    "            # Input interactivo\n",
    "            try:\n",
    "                confirm = input(f\"ADVERTENCIA: Esta acción puede impactar rendimiento. Escribe 'BORRAR INDICES USUARIO' para confirmar: \")\n",
    "            except EOFError:\n",
    "                 logger.error(\"No se pudo obtener confirmación interactiva para delete_all_user_indexes.\")\n",
    "                 print(\"ERROR: No se puede confirmar borrado en entorno no interactivo. Operación cancelada.\")\n",
    "                 return False\n",
    "\n",
    "            if confirm != \"BORRAR INDICES USUARIO\":\n",
    "                logger.warning(f\"Borrado de índices cancelado por usuario para BD '{effective_db}'.\")\n",
    "                print(\"Borrado de índices cancelado.\")\n",
    "                return False\n",
    "\n",
    "            logger.warning(f\"PROCEDIENDO A BORRAR {len(index_names_to_delete)} índices en BD '{effective_db}'...\")\n",
    "            deleted_count = 0\n",
    "            errors = []\n",
    "            start_time = time.time()\n",
    "            for index_name in index_names_to_delete:\n",
    "                # Verificar si el índice todavía existe antes de intentar borrarlo\n",
    "                check_exists_query = \"SHOW INDEXES WHERE name = $index_name RETURN name\"\n",
    "                exists_result = session.run(check_exists_query, parameters={\"index_name\": index_name}).single()\n",
    "\n",
    "                if exists_result:\n",
    "                    try:\n",
    "                        logger.debug(f\"Ejecutando DROP INDEX `{index_name}`...\")\n",
    "                        summary = session.run(f\"DROP INDEX `{index_name}`\").consume()\n",
    "                        logger.debug(f\"Índice '{index_name}' eliminado de BD '{effective_db}'. Contadores: {summary.counters}\")\n",
    "                        deleted_count += 1\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error eliminando índice '{index_name}' BD '{effective_db}': {e}\", exc_info=True)\n",
    "                        errors.append(index_name)\n",
    "                else:\n",
    "                    logger.debug(f\"Índice '{index_name}' no encontrado, omitiendo borrado.\")\n",
    "                    # Contar como procesado aunque no existiera\n",
    "                    deleted_count += 1\n",
    "\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "            if not errors:\n",
    "                logger.info(f\"{deleted_count} índices procesados para borrado en {duration:.2f}s de BD '{effective_db}'.\")\n",
    "                print(f\"Se procesó el borrado de {deleted_count} índices de usuario de '{effective_db}'.\")\n",
    "                return True\n",
    "            else:\n",
    "                logger.error(f\"Fallo al eliminar índices en BD '{effective_db}': {', '.join(errors)}\")\n",
    "                print(f\"Error al borrar índices en '{effective_db}'. Ver logs.\")\n",
    "                return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error durante eliminación de índices BD '{effective_db}': {e}\", exc_info=True)\n",
    "        print(f\"Error general al procesar índices en '{effective_db}'. Ver logs.\")\n",
    "        return False\n",
    "\n",
    "def display_neo4j_browser(browser_url: str = 'http://localhost:7474/browser/'):\n",
    "    \"\"\"Intenta abrir el navegador web en la URL de Neo4j Browser.\"\"\"\n",
    "    try:\n",
    "        is_opened = webbrowser.open(browser_url, new=2) # new=2 intenta nueva pestaña\n",
    "        if is_opened:\n",
    "             logger.info(f\"Navegador abierto (o intentado) en: {browser_url}\")\n",
    "             print(f\"Se intentó abrir el navegador en {browser_url}\")\n",
    "        else:\n",
    "             # Esto puede pasar si no hay navegador gráfico o por permisos\n",
    "             logger.warning(f\"webbrowser.open devolvió False para {browser_url}. Puede que no se haya abierto.\")\n",
    "             print(f\"No se pudo confirmar apertura automática del navegador en {browser_url}. Abre la URL manualmente si es necesario.\")\n",
    "        return is_opened\n",
    "    except Exception as e:\n",
    "        logger.error(f\"No se pudo abrir navegador para Neo4j Browser: {e}\", exc_info=True)\n",
    "        print(f\"Error al intentar abrir el navegador: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"Funciones utilitarias de Neo4j definidas.\")\n",
    "logger.info(\"Bloque 5: Funciones utilitarias de Neo4j completadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07834ad5-b039-46f9-b45d-af66620e3570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:48:13,177 - INFO - [__main__:4] - Bloque 6: Definiendo plantillas de prompts...\n",
      "2025-04-27 15:48:13,181 - INFO - [__main__:118] - Bloque 6: Plantillas de prompts definidas.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt para chunking (Ollama JSON) definido.\n",
      "Prompt para extracción de grafo JSON (LLM Principal) definido.\n",
      "Prompt para consulta del grafo (Neo4j opcional) definido.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# %% Bloque 6: Definición de Prompts\n",
    "# ============================================================\n",
    "logger.info(\"Bloque 6: Definiendo plantillas de prompts...\")\n",
    "\n",
    "# --- Plantilla para Chunking Semántico (Usada por Ollama JSON) ---\n",
    "prompt_chunking_semantic_template = PromptTemplate.from_template(\n",
    "    template=\"\"\"Eres un experto en NLP. Divide el texto en chunks semánticamente coherentes y autocontenidos. Sigue estas reglas ESTRICTAMENTE:\n",
    "1. Descomposición Proposicional: Divide en las proposiciones (ideas) más simples posibles.\n",
    "2. Oraciones Simples: Convierte oraciones complejas en varias simples. Mantén redacción original si puedes.\n",
    "3. Separación de Descripciones: Info descriptiva de entidades -> chunk propio.\n",
    "4. Descontextualización: Reemplaza pronombres (él, ella, esto) con nombres completos. Añade contexto si es necesario para que el chunk se entienda solo.\n",
    "5. Coherencia: Cada chunk debe ser gramaticalmente correcto.\n",
    "6. IDs: Asigna ID numérico secuencial a cada chunk (desde 1).\n",
    "\n",
    "Texto de Entrada:\n",
    "--------------------\n",
    "{input}\n",
    "--------------------\n",
    "\n",
    "Instrucción Final: Prepara tu respuesta como JSON siguiendo el esquema [{{\"chunk_id\": int, \"text\": string}}]. El sistema forzará este formato, tú genera el contenido correcto.\"\"\"\n",
    ")\n",
    "\n",
    "# --- Plantilla para Extracción de Grafo (Usada por LLM Principal) ---\n",
    "# (Usa el definido en el Bloque 3 para ExtractedGraph)\n",
    "prompt_graph_extraction_template = PromptTemplate.from_template(\n",
    "    template=\"\"\"Eres un experto analista de texto y modelador de grafos de conocimiento. Tu tarea es leer el texto proporcionado y extraer una estructura de grafo significativa.\n",
    "\n",
    "**Instrucciones Detalladas:**\n",
    "\n",
    "1.  **Identifica Nodos:**\n",
    "    *   Extrae las entidades clave (Personas, Organizaciones, Lugares, Proyectos, Conceptos técnicos, Productos, Fechas importantes, etc.).\n",
    "    *   Para cada nodo, asigna:\n",
    "        *   `id`: Un identificador **único y canónico**. Usa el nombre completo o término más representativo y normalizado (e.g., \"Microsoft Corp.\" en lugar de \"Microsoft\" o \"MSFT\"). Sé consistente.\n",
    "        *   `label`: Una etiqueta descriptiva y concisa en formato PascalCase (e.g., \"Person\", \"Organization\", \"Concept\", \"Location\", \"Project\", \"Product\", \"Date\", \"Technology\").\n",
    "        *   `properties`: Un diccionario de propiedades adicionales extraídas **directamente** del texto (e.g., para \"Person\": {{\"title\": \"CEO\"}}, para \"Product\": {{\"version\": \"1.0\"}}). Incluye solo propiedades explícitas. Usa claves en minúscula.\n",
    "\n",
    "2.  **Identifica Relaciones:**\n",
    "    *   Extrae las relaciones significativas *entre los nodos que identificaste*.\n",
    "    *   Para cada relación, asigna:\n",
    "        *   `source`: El `id` del nodo origen (debe coincidir con un `id` de nodo extraído).\n",
    "        *   `target`: El `id` del nodo destino (debe coincidir con un `id` de nodo extraído).\n",
    "        *   `type`: Un tipo de relación claro en formato VERBO_EN_MAYUSCULAS (e.g., \"WORKS_AT\", \"LOCATED_IN\", \"MENTIONS\", \"DISCUSSES\", \"PART_OF\", \"HAS_FEATURE\", \"ANNOUNCED_ON\", \"RELATED_TO\").\n",
    "        *   `properties`: Un diccionario de propiedades de la relación, si las hay (e.g., {{\"role\": \"Developer\"}}, {{\"date\": \"2024-01-15\"}}).\n",
    "\n",
    "3.  **Contexto y Precisión:** Basa la extracción *estrictamente* en el texto proporcionado. No inventes nodos, relaciones o propiedades. Si una entidad o relación no es clara, es mejor omitirla.\n",
    "4.  **Formato de Salida:** Estructura tu respuesta **exactamente** como un objeto JSON que cumpla con el esquema Pydantic proporcionado. Asegúrate de que el JSON sea válido, incluyendo comillas dobles correctas y comas donde sea necesario.\n",
    "\n",
    "**Texto a Analizar:**\n",
    "--------------------\n",
    "{chunk_text}\n",
    "--------------------\n",
    "\n",
    "**Esquema JSON Requerido (Sigue esta estructura):**\n",
    "```json\n",
    "{{\n",
    "  \"nodes\": [\n",
    "    {{\n",
    "      \"id\": \"id_canonico_nodo_1\",\n",
    "      \"label\": \"EtiquetaPascalCase\",\n",
    "      \"properties\": {{ \"propiedad_minuscula\": \"valor\" }}\n",
    "    }},\n",
    "    ...\n",
    "  ],\n",
    "  \"relationships\": [\n",
    "    {{\n",
    "      \"source\": \"id_nodo_origen\",\n",
    "      \"target\": \"id_nodo_destino\",\n",
    "      \"type\": \"TIPO_RELACION_MAYUSCULAS\",\n",
    "      \"properties\": {{ \"prop_rel_minuscula\": \"valor_rel\" }}\n",
    "    }},\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "{format_instructions}\n",
    "\n",
    "Salida JSON (Solo el objeto JSON, nada más antes o después):\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "#--- Plantilla para Consulta del Grafo (Usada si load_into_neo4j=True) ---\n",
    "\n",
    "prompt_graph_query_template = PromptTemplate.from_template(\n",
    "template=\"\"\"Eres un asistente experto en grafos de conocimiento Neo4j. Responde preguntas basándote únicamente en el esquema y resumen del grafo proporcionados.\n",
    "\n",
    "Instrucciones Clave:\n",
    "\n",
    "    Usa Solo el Esquema/Resumen: No inventes información no presente o sugerida.\n",
    "\n",
    "    Interpreta el Esquema: Entiende qué tipos de nodos (Labels) y relaciones existen.\n",
    "\n",
    "    Consulta el Resumen: Obtén una idea del contenido actual (nodos, relaciones).\n",
    "\n",
    "    Si Falta Información: Indica claramente que no puedes responder con los datos dados (ej: \"El esquema no contiene información sobre X...\"). No adivines.\n",
    "\n",
    "    Sé Conciso: Responde directamente a la pregunta.\n",
    "\n",
    "Esquema y Resumen del Grafo:\n",
    "\n",
    "Esquema:\n",
    "{knowledge_graph_schema}\n",
    "Resumen:\n",
    "{knowledge_graph_summary}\n",
    "\n",
    "Pregunta del Usuario: {question}\n",
    "\n",
    "Respuesta:\"\"\"\n",
    ")\n",
    "#--- Selección Final de Prompts ---\n",
    "\n",
    "prompt_chunking = prompt_chunking_semantic_template # Usado internamente por GraphProcessor para Ollama\n",
    "prompt_extraction = prompt_graph_extraction_template # Usado por GraphProcessor para el LLM principal\n",
    "prompt_query = prompt_graph_query_template # Para consultas opcionales a Neo4j\n",
    "\n",
    "print(f\"Prompt para chunking (Ollama JSON) definido.\")\n",
    "print(f\"Prompt para extracción de grafo JSON (LLM Principal) definido.\")\n",
    "print(f\"Prompt para consulta del grafo (Neo4j opcional) definido.\")\n",
    "logger.info(\"Bloque 6: Plantillas de prompts definidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "197081c5-697b-4377-b195-f22edc808017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:48:15,542 - INFO - [__main__:4] - Bloque 7: Definiendo la clase GraphProcessor...\n",
      "2025-04-27 15:48:15,548 - INFO - [__main__:631] - Bloque 7: Definición de GraphProcessor completada.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase GraphProcessor definida.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ============================================================\n",
    "# %% Bloque 7: Definición de la Clase GraphProcessor\n",
    "# ============================================================\n",
    "logger.info(\"Bloque 7: Definiendo la clase GraphProcessor...\")\n",
    "\n",
    "class GraphProcessor:\n",
    "    \"\"\"\n",
    "    Orquesta el procesamiento de archivos para extraer estructuras de grafo en JSON.\n",
    "    Opcionalmente, carga la estructura extraída a Neo4j.\n",
    "\n",
    "    Utiliza:\n",
    "    - LLM Ollama (configurado específicamente) para chunking JSON robusto.\n",
    "    - LLM Principal (configurable) para extraer la estructura del grafo (Nodos/Relaciones)\n",
    "      a partir de los chunks y formatearla como JSON según el esquema `ExtractedGraph`.\n",
    "    - Opcionalmente, interactúa con Neo4j para cargar datos y realizar consultas.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 main_llm: BaseChatModel,       # LLM principal (para extracción JSON)\n",
    "                 graph_instance: Optional[Neo4jGraph], # Instancia Neo4jGraph (SOLO si se carga/consulta)\n",
    "                 chunking_prompt: PromptTemplate, # Prompt semántico para chunking\n",
    "                 extraction_prompt: PromptTemplate,# Prompt para extraer grafo JSON\n",
    "                 ollama_chunking_model: str,     # Modelo Ollama *para chunking*\n",
    "                 ollama_url: str,                # URL Ollama *para chunking*\n",
    "                 chunk_schema: type[BaseModel] = Chunks,       # Schema Pydantic para chunks\n",
    "                 graph_schema: type[BaseModel] = ExtractedGraph # Schema Pydantic para grafo extraído\n",
    "                ):\n",
    "        \"\"\"Inicializa el GraphProcessor.\"\"\"\n",
    "\n",
    "        if not main_llm: raise ValueError(\"Se requiere instancia del LLM principal (main_llm).\")\n",
    "        self.llm = main_llm\n",
    "        logger.info(f\"GraphProcessor inicializado con LLM principal tipo: {type(main_llm)}\")\n",
    "\n",
    "        self.graph = graph_instance # Puede ser None\n",
    "        if self.graph: logger.info(\"Instancia Neo4jGraph proporcionada (para carga/consulta opcional).\")\n",
    "        else: logger.info(\"Instancia Neo4jGraph NO proporcionada (solo se generará JSON).\")\n",
    "\n",
    "        if not chunking_prompt: raise ValueError(\"Se requiere plantilla de prompt para chunking.\")\n",
    "        if not extraction_prompt: raise ValueError(\"Se requiere plantilla de prompt para extracción de grafo.\")\n",
    "\n",
    "        # --- Configuración Cadena de Chunking (SIEMPRE Ollama JSON) ---\n",
    "        self.chunking_chain = None\n",
    "        try:\n",
    "            logger.info(f\"Configurando LLM Ollama para chunking (Modelo: {ollama_chunking_model})...\")\n",
    "            # Añadir timeout a la llamada Ollama\n",
    "            llm_chunking = ChatOllama(model=ollama_chunking_model, base_url=ollama_url, format=\"json\", temperature=0.0, request_timeout=120.0) # Timeout 2 min\n",
    "            chunking_parser = PydanticOutputParser(pydantic_object=chunk_schema)\n",
    "            chunking_prompt_formatted = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"Responde SIEMPRE usando formato JSON válido según el esquema.\\n{format_instructions}\"),\n",
    "                (\"human\", chunking_prompt.template)\n",
    "            ]).partial(format_instructions=chunking_parser.get_format_instructions())\n",
    "            # Usar OutputFixingParser con el mismo LLM para robustez\n",
    "            output_fixing_parser_chunking = OutputFixingParser.from_llm(parser=chunking_parser, llm=llm_chunking)\n",
    "            self.chunking_chain = chunking_prompt_formatted | llm_chunking | output_fixing_parser_chunking\n",
    "            logger.info(\"Cadena de Chunking (Ollama JSON + Fixer) configurada.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fallo crítico configurando cadena de chunking Ollama: {e}\", exc_info=True)\n",
    "            raise Exception(f\"Fallo en configuración de chunking Ollama: {e}\") from e\n",
    "\n",
    "        # --- Configuración Cadena de Extracción de Grafo (LLM Principal + Pydantic) ---\n",
    "        self.graph_extraction_chain = None\n",
    "        try:\n",
    "            logger.info(f\"Configurando cadena de extracción de grafo con LLM principal ({type(self.llm)})...\")\n",
    "            graph_parser = PydanticOutputParser(pydantic_object=graph_schema)\n",
    "\n",
    "            # Crear el prompt completo incluyendo las instrucciones de formato Pydantic\n",
    "            # Pasar `format_instructions` como variable al template\n",
    "            self.extraction_prompt_formatted = PromptTemplate(\n",
    "                template=extraction_prompt.template, # Usar el template original\n",
    "                input_variables=[\"chunk_text\"], # Variables que el usuario proveerá\n",
    "                partial_variables={\"format_instructions\": graph_parser.get_format_instructions()} # Instrucciones Pydantic\n",
    "            )\n",
    "\n",
    "            # Crear la cadena: Prompt Formateado -> LLM -> Parser Pydantic\n",
    "            # Añadir OutputFixingParser si se espera que el LLM falle mucho con JSON\n",
    "            # graph_fixing_parser = OutputFixingParser.from_llm(parser=graph_parser, llm=self.llm)\n",
    "            # self.graph_extraction_chain = self.extraction_prompt_formatted | self.llm | graph_fixing_parser\n",
    "            self.graph_extraction_chain = self.extraction_prompt_formatted | self.llm | graph_parser\n",
    "            logger.info(\"Cadena de Extracción de Grafo (LLM Principal + Pydantic Parser) configurada.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fallo crítico configurando cadena de extracción de grafo: {e}\", exc_info=True)\n",
    "            raise Exception(f\"Fallo en configuración de extracción de grafo: {e}\") from e\n",
    "\n",
    "\n",
    "    def _read_file(self, filepath: str) -> Optional[str]:\n",
    "        \"\"\"Lee contenido de archivo, probando codificaciones comunes.\"\"\"\n",
    "        encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
    "        logger.debug(f\"Intentando leer: {filepath}\")\n",
    "        try:\n",
    "             # Comprobar si el archivo existe primero\n",
    "            if not os.path.isfile(filepath):\n",
    "                logger.error(f\"Archivo no encontrado en la ruta especificada: {filepath}\")\n",
    "                return None\n",
    "            for enc in encodings:\n",
    "                try:\n",
    "                    with open(filepath, \"r\", encoding=enc) as f:\n",
    "                        # Leer todo el contenido, luego procesar\n",
    "                        full_content = f.read()\n",
    "                        # Unir líneas no vacías después de quitar espacios al inicio/fin\n",
    "                        # y reemplazar múltiples espacios con uno solo\n",
    "                        content = ' '.join(line.strip() for line in full_content.splitlines() if line.strip())\n",
    "                        content = ' '.join(content.split()) # Normalizar espacios\n",
    "                    logger.info(f\"Archivo leído: {filepath} (codificación: {enc})\")\n",
    "                    return content\n",
    "                except UnicodeDecodeError:\n",
    "                    logger.debug(f\"Fallo con codificación {enc} para {filepath}, intentando siguiente...\")\n",
    "                    continue\n",
    "                except IOError as e: # Capturar errores de permiso, etc. aquí\n",
    "                    logger.error(f\"Error I/O leyendo {filepath} (permisos? disco lleno?): {e}\")\n",
    "                    return None # No seguir intentando si hay error de IO\n",
    "\n",
    "            # Si salimos del bucle sin éxito en ninguna codificación\n",
    "            logger.error(f\"No se pudo decodificar el archivo {filepath} con las codificaciones probadas: {encodings}\")\n",
    "            return None\n",
    "        except Exception as e: # Capturar otros errores inesperados\n",
    "            logger.error(f\"Error inesperado accediendo/leyendo archivo {filepath}: {e}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "\n",
    "    def _chunk_text_with_ollama_json(self, text: str) -> Optional[List[Chunk]]:\n",
    "        \"\"\"Divide texto en chunks usando cadena Ollama JSON.\"\"\"\n",
    "        if not self.chunking_chain:\n",
    "            logger.error(\"Cadena chunking (Ollama) no inicializada. No se puede dividir.\")\n",
    "            return None\n",
    "        if not text or not text.strip():\n",
    "            logger.warning(\"Texto para chunking vacío o solo espacios.\")\n",
    "            return []\n",
    "\n",
    "        logger.info(\"Iniciando división de texto con Ollama (modo JSON)...\")\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            logger.debug(f\"Invocando cadena chunking Ollama con texto (len={len(text)} chars)...\")\n",
    "            # El parser (con fixer) debería devolver un objeto Chunks o lanzar excepción\n",
    "            response: Chunks = self.chunking_chain.invoke({\"input\": text})\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            if isinstance(response, Chunks) and hasattr(response, 'chunks'):\n",
    "                num_chunks = len(response.chunks)\n",
    "                logger.info(f\"Chunking Ollama completado ({num_chunks} chunks) en {duration:.2f}s.\")\n",
    "                # Validar IDs secuenciales (opcional)\n",
    "                for i, chk in enumerate(response.chunks):\n",
    "                     if chk.chunk_id != i + 1:\n",
    "                         logger.warning(f\"Chunk ID no secuencial detectado: Esperado {i+1}, Obtenido {chk.chunk_id}. Se usará el obtenido.\")\n",
    "                return response.chunks\n",
    "            else:\n",
    "                # Esto teóricamente no debería ocurrir si el parser funciona/lanza excepción\n",
    "                logger.error(f\"Chunking Ollama devolvió tipo inesperado tras parseo/fix: {type(response)}. Contenido parcial: {str(response)[:500]}...\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            duration = time.time() - start_time\n",
    "            # Loguear el traceback completo\n",
    "            tb_str = traceback.format_exc()\n",
    "            logger.error(f\"Error DETALLADO durante chunking Ollama (después de {duration:.2f}s): {e}\\nTraceback:\\n{tb_str}\")\n",
    "            # Podrías querer guardar el texto problemático para depuración\n",
    "            # try:\n",
    "            #     with open(\"error_chunking_input.txt\", \"w\", encoding='utf-8') as f_err:\n",
    "            #         f_err.write(text)\n",
    "            #     logger.info(\"Texto de entrada problemático guardado en error_chunking_input.txt\")\n",
    "            # except Exception as save_e:\n",
    "            #     logger.error(f\"No se pudo guardar el texto de entrada del error: {save_e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def _extract_graph_from_chunk(self, chunk: Chunk) -> Optional[ExtractedGraph]:\n",
    "        \"\"\"Extrae estructura de grafo (JSON) de un chunk usando LLM principal.\"\"\"\n",
    "        if not self.graph_extraction_chain:\n",
    "            logger.error(\"Cadena de extracción de grafo no inicializada.\")\n",
    "            return None\n",
    "        if not chunk or not chunk.text or not chunk.text.strip():\n",
    "             logger.warning(f\"Chunk {chunk.chunk_id if chunk else 'N/A'} está vacío. Saltando extracción.\")\n",
    "             return ExtractedGraph(nodes=[], relationships=[]) # Devolver grafo vacío\n",
    "\n",
    "        logger.info(f\"Iniciando extracción de grafo del chunk ID: {chunk.chunk_id} (len={len(chunk.text)} chars)...\")\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # Invocar la cadena de extracción\n",
    "            extracted_data: ExtractedGraph = self.graph_extraction_chain.invoke({\n",
    "                \"chunk_text\": chunk.text\n",
    "            })\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            # Validar resultado (Pydantic lo hace, pero podemos añadir más checks)\n",
    "            if isinstance(extracted_data, ExtractedGraph):\n",
    "                 node_count = len(extracted_data.nodes)\n",
    "                 rel_count = len(extracted_data.relationships)\n",
    "                 logger.info(f\"Extracción grafo chunk {chunk.chunk_id} completada ({duration:.2f}s). Nodos: {node_count}, Rels: {rel_count}.\")\n",
    "                 # Validar que las props sean diccionarios (Pydantic debería asegurarlo, pero doble check)\n",
    "                 for node in extracted_data.nodes: node.properties = node.properties or {}\n",
    "                 for rel in extracted_data.relationships: rel.properties = rel.properties or {}\n",
    "                 return extracted_data\n",
    "            else: # Si el parser devuelve algo inesperado (poco probable con PydanticParser directo)\n",
    "                 logger.error(f\"Extracción grafo chunk {chunk.chunk_id} devolvió tipo inesperado: {type(extracted_data)}\")\n",
    "                 return None\n",
    "\n",
    "        except ValidationError as e: # Error de validación Pydantic (JSON inválido del LLM)\n",
    "             duration = time.time() - start_time\n",
    "             logger.error(f\"Error Validación Pydantic (JSON malformado?) en extracción chunk {chunk.chunk_id} ({duration:.2f}s): {e}\", exc_info=False)\n",
    "             # Intentar loguear la salida cruda si se puede (requiere ajuste en Langchain o try/except en parser)\n",
    "             # logger.error(f\"Salida cruda LLM (si disponible): ...\")\n",
    "             return None\n",
    "        except Exception as e: # Otros errores (API, conexión LLM, timeout, etc.)\n",
    "            duration = time.time() - start_time\n",
    "            tb_str = traceback.format_exc()\n",
    "            logger.error(f\"Error DETALLADO en extracción chunk {chunk.chunk_id} ({duration:.2f}s): {e}\\nTraceback:\\n{tb_str}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def _aggregate_extracted_graphs(self, graph_parts: List[Optional[ExtractedGraph]]) -> Dict[str, Any]:\n",
    "        \"\"\"Combina nodos y relaciones de múltiples extracciones, eliminando duplicados y resolviendo conflictos simples.\"\"\"\n",
    "        aggregated_nodes: Dict[str, Node] = {} # Clave: node.id\n",
    "        aggregated_relationships_list: List[Relationship] = [] # Guardar objetos para merge de props\n",
    "\n",
    "        logger.debug(f\"Agregando grafos de {len(graph_parts)} partes extraídas...\")\n",
    "        part_count = 0\n",
    "        for part in filter(None, graph_parts): # Filtrar Nones y asegurar que es ExtractedGraph\n",
    "             part_count += 1\n",
    "             if not isinstance(part, ExtractedGraph):\n",
    "                  logger.warning(f\"Elemento inesperado encontrado durante agregación (tipo: {type(part)}). Omitiendo.\")\n",
    "                  continue\n",
    "\n",
    "             # Agregar/Actualizar Nodos\n",
    "             for node in part.nodes:\n",
    "                 if not node.id or not node.label:\n",
    "                      logger.warning(f\"Nodo inválido omitido en parte {part_count}: ID='{node.id}', Label='{node.label}'\")\n",
    "                      continue\n",
    "                 # Normalizar ID (ej. quitar espacios extra) puede ser útil aquí\n",
    "                 node_id_norm = node.id.strip()\n",
    "                 if not node_id_norm: continue # Saltar si ID queda vacío\n",
    "\n",
    "                 if node_id_norm in aggregated_nodes:\n",
    "                     # Fusionar propiedades: las nuevas tienen prioridad\n",
    "                     existing_node = aggregated_nodes[node_id_norm]\n",
    "                     merged_props = existing_node.properties.copy()\n",
    "                     merged_props.update(node.properties or {}) # Asegurar que node.properties no sea None\n",
    "                     existing_node.properties = merged_props\n",
    "                     # Etiqueta: mantener la primera vista o implementar lógica de mayoría/prioridad\n",
    "                     if existing_node.label != node.label:\n",
    "                          logger.debug(f\"Conflicto etiqueta nodo ID '{node_id_norm}': '{existing_node.label}' vs '{node.label}'. Se mantiene '{existing_node.label}'.\")\n",
    "                 else:\n",
    "                     # Añadir nodo nuevo\n",
    "                     node.id = node_id_norm # Usar ID normalizado\n",
    "                     node.properties = node.properties or {} # Asegurar dict\n",
    "                     aggregated_nodes[node_id_norm] = node\n",
    "\n",
    "             # Agregar Relaciones (manejo simple de duplicados)\n",
    "             for rel in part.relationships:\n",
    "                 # Validar relación básica y normalizar IDs\n",
    "                 source_id_norm = rel.source.strip() if rel.source else None\n",
    "                 target_id_norm = rel.target.strip() if rel.target else None\n",
    "                 rel_type = rel.type.strip() if rel.type else None\n",
    "\n",
    "                 if not source_id_norm or not target_id_norm or not rel_type:\n",
    "                      logger.warning(f\"Relación inválida omitida en parte {part_count}: {source_id_norm} -[{rel_type}]-> {target_id_norm}\")\n",
    "                      continue\n",
    "                 # Verificar si nodos existen (importante para consistencia)\n",
    "                 if source_id_norm not in aggregated_nodes or target_id_norm not in aggregated_nodes:\n",
    "                      logger.warning(f\"Relación omitida: Nodo source ('{source_id_norm}') o target ('{target_id_norm}') no encontrado. Rel: {rel_type}\")\n",
    "                      continue\n",
    "\n",
    "                 # Buscar si ya existe relación igual (mismo source, target, type)\n",
    "                 found_existing = False\n",
    "                 for i, existing_rel in enumerate(aggregated_relationships_list):\n",
    "                      if existing_rel.source == source_id_norm and existing_rel.target == target_id_norm and existing_rel.type == rel_type:\n",
    "                          # Fusionar propiedades\n",
    "                          merged_props = existing_rel.properties.copy()\n",
    "                          merged_props.update(rel.properties or {})\n",
    "                          aggregated_relationships_list[i].properties = merged_props\n",
    "                          found_existing = True\n",
    "                          logger.debug(f\"Relación existente actualizada: {source_id_norm}-[{rel_type}]->{target_id_norm}\")\n",
    "                          break\n",
    "                 if not found_existing:\n",
    "                      # Añadir nueva relación usando IDs normalizados\n",
    "                      rel.source = source_id_norm\n",
    "                      rel.target = target_id_norm\n",
    "                      rel.type = rel_type\n",
    "                      rel.properties = rel.properties or {} # Asegurar dict\n",
    "                      aggregated_relationships_list.append(rel)\n",
    "\n",
    "        # Convertir a formato serializable (dict)\n",
    "        final_structure = {\n",
    "            \"nodes\": [node.dict() for node in aggregated_nodes.values()],\n",
    "            \"relationships\": [rel.dict() for rel in aggregated_relationships_list]\n",
    "        }\n",
    "        logger.info(f\"Agregación completada. Nodos únicos: {len(final_structure['nodes'])}, Relaciones únicas: {len(final_structure['relationships'])}.\")\n",
    "        return final_structure\n",
    "\n",
    "\n",
    "    def _save_json(self, data: Dict[str, Any], filepath: str):\n",
    "        \"\"\"Guarda los datos en un archivo JSON con formato legible.\"\"\"\n",
    "        logger.info(f\"Intentando guardar JSON en: {filepath}\")\n",
    "        try:\n",
    "            # Asegurar que el directorio existe\n",
    "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=2) # indent=2 para legibilidad\n",
    "            logger.info(f\"Estructura del grafo guardada exitosamente.\")\n",
    "            return True\n",
    "        except IOError as e:\n",
    "            logger.error(f\"Error de E/S al guardar JSON en {filepath}: {e}\")\n",
    "        except TypeError as e:\n",
    "            logger.error(f\"Error de tipo al serializar JSON para {filepath} (objeto no serializable?): {e}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error inesperado al guardar JSON en {filepath}: {e}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "\n",
    "    def _visualize_json(self, data: Dict[str, Any], max_items: int = 10):\n",
    "        \"\"\"Imprime una vista previa del JSON en la consola/notebook.\"\"\"\n",
    "        try:\n",
    "            print(\"\\n--- Visualización Previa JSON Extraído ---\")\n",
    "            if not isinstance(data, dict):\n",
    "                 print(\"Error: Datos inválidos para visualizar.\")\n",
    "                 return\n",
    "\n",
    "            nodes = data.get(\"nodes\", [])\n",
    "            rels = data.get(\"relationships\", [])\n",
    "\n",
    "            # Crear una copia para la vista previa\n",
    "            preview_data = {\n",
    "                \"nodes\": nodes[:max_items],\n",
    "                \"relationships\": rels[:max_items]\n",
    "            }\n",
    "\n",
    "            print(json.dumps(preview_data, indent=2, ensure_ascii=False))\n",
    "\n",
    "            if len(nodes) > max_items:\n",
    "                print(f\"  (... {len(nodes) - max_items} nodos más ...)\")\n",
    "            if len(rels) > max_items:\n",
    "                print(f\"  (... {len(rels) - max_items} relaciones más ...)\")\n",
    "            print(f\"--- (Mostrando hasta {max_items} nodos/relaciones. Total: {len(nodes)} N, {len(rels)} R) ---\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error al visualizar JSON: {e}\")\n",
    "            print(\"Error al visualizar JSON.\")\n",
    "\n",
    "\n",
    "    def process_file_to_json(self, filepath: str, output_dir: str, print_chunks_flag: bool, visualize_json_flag: bool) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Procesa un archivo: lee, chunkea (Ollama), extrae grafo (LLM principal),\n",
    "        agrega resultados, guarda JSON y opcionalmente lo visualiza.\n",
    "        Devuelve la estructura de grafo agregada como diccionario, o None si falla gravemente.\n",
    "        \"\"\"\n",
    "        logger.info(f\"--- Iniciando procesamiento del archivo: {filepath} ---\")\n",
    "        file_basename = os.path.basename(filepath)\n",
    "\n",
    "        # 1. Leer Archivo\n",
    "        content = self._read_file(filepath)\n",
    "        if not content:\n",
    "            logger.error(f\"No se pudo leer {file_basename}. Saltando.\")\n",
    "            return None\n",
    "\n",
    "        # 2. Chunking (Ollama JSON)\n",
    "        chunks = self._chunk_text_with_ollama_json(content)\n",
    "        if chunks is None:\n",
    "            logger.error(f\"Fallo en chunking para {file_basename}. Saltando extracción.\")\n",
    "            return None\n",
    "        if not chunks:\n",
    "            logger.warning(f\"No se generaron chunks para {file_basename}. No se extraerá grafo.\")\n",
    "            empty_graph = {\"nodes\": [], \"relationships\": []}\n",
    "            output_filename = os.path.splitext(file_basename)[0] + \".graph.json\"\n",
    "            output_filepath = os.path.join(output_dir, output_filename)\n",
    "            self._save_json(empty_graph, output_filepath) # Guardar archivo vacío\n",
    "            return empty_graph # Indicar que se procesó pero no hubo contenido\n",
    "\n",
    "        if print_chunks_flag:\n",
    "            print(f\"\\n--- Chunks (Ollama) para {file_basename} ({len(chunks)}) ---\")\n",
    "            for chk in chunks[:5]: print(f\"ID: {chk.chunk_id}, Texto: '{chk.text[:80].strip()}...'\")\n",
    "            if len(chunks) > 5: print(\"  (...)\")\n",
    "\n",
    "        # 3. Extracción de Grafo por Chunk (LLM Principal)\n",
    "        extracted_graph_parts: List[Optional[ExtractedGraph]] = []\n",
    "        total_chunks = len(chunks)\n",
    "        logger.info(f\"Iniciando extracción de grafo para {total_chunks} chunks de {file_basename}...\")\n",
    "        start_extraction_time = time.time()\n",
    "        successful_extractions = 0\n",
    "        failed_extractions = 0\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            current_time = time.time()\n",
    "            if (i + 1) % 20 == 0 or current_time - start_extraction_time > 90: # Log cada 20 chunks o 90 segs\n",
    "                 elapsed_time = current_time - start_extraction_time\n",
    "                 logger.info(f\"  Progreso extracción {file_basename}: Chunk {i+1}/{total_chunks}... ({elapsed_time:.1f}s desde último log)\")\n",
    "                 start_extraction_time = current_time # Reset timer\n",
    "\n",
    "            graph_part = self._extract_graph_from_chunk(chunk)\n",
    "            if graph_part is not None:\n",
    "                # Solo añadir si tiene nodos o relaciones\n",
    "                if graph_part.nodes or graph_part.relationships:\n",
    "                     extracted_graph_parts.append(graph_part)\n",
    "                     successful_extractions += 1\n",
    "                else:\n",
    "                     logger.debug(f\"Extracción chunk {chunk.chunk_id} resultó en grafo vacío. Omitiendo.\")\n",
    "                     # Contar como éxito si no hubo error, aunque esté vacío\n",
    "                     successful_extractions +=1 # O decidir no contarlo\n",
    "            else: # Hubo un error en la extracción\n",
    "                failed_extractions += 1\n",
    "                # El error ya se logueó en _extract_graph_from_chunk\n",
    "\n",
    "        logger.info(f\"Extracción de chunks para {file_basename} completada. Exitosos/Intentados: {successful_extractions}/{total_chunks}, Fallos: {failed_extractions}.\")\n",
    "\n",
    "        # Decidir si continuar si hubo muchos fallos\n",
    "        # if failed_extractions > total_chunks * 0.5: # Ejemplo: si más del 50% fallaron\n",
    "        #      logger.error(f\"Demasiados fallos ({failed_extractions}) en extracción para {file_basename}. Abortando agregación/guardado.\")\n",
    "        #      return None\n",
    "\n",
    "        if not extracted_graph_parts:\n",
    "             logger.error(f\"No se pudo extraer ninguna estructura de grafo válida (con nodos/rels) para {file_basename}.\")\n",
    "             empty_graph = {\"nodes\": [], \"relationships\": []}\n",
    "             output_filename = os.path.splitext(file_basename)[0] + \".graph.json\"\n",
    "             output_filepath = os.path.join(output_dir, output_filename)\n",
    "             self._save_json(empty_graph, output_filepath)\n",
    "             return None # Indicar fallo global en extracción\n",
    "\n",
    "        # 4. Agregar Resultados\n",
    "        logger.info(f\"Agregando resultados de {len(extracted_graph_parts)} partes extraídas para {file_basename}...\")\n",
    "        aggregated_graph_data = self._aggregate_extracted_graphs(extracted_graph_parts)\n",
    "\n",
    "        # 5. Guardar JSON Agregado\n",
    "        output_filename = os.path.splitext(file_basename)[0] + \".graph.json\"\n",
    "        output_filepath = os.path.join(output_dir, output_filename)\n",
    "        save_success = self._save_json(aggregated_graph_data, output_filepath)\n",
    "\n",
    "        # 6. Visualizar JSON (Opcional)\n",
    "        if visualize_json_flag and save_success:\n",
    "            self._visualize_json(aggregated_graph_data)\n",
    "\n",
    "        logger.info(f\"--- Procesamiento JSON para archivo {file_basename} completado ---\")\n",
    "        return aggregated_graph_data # Devolver datos agregados\n",
    "\n",
    "    # (Dentro de la clase GraphProcessor en Bloque 7)\n",
    "\n",
    "    def _load_graph_data_to_neo4j(self, graph_data: Dict[str, Any], driver: Driver, db_name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Carga la estructura de grafo JSON a Neo4j usando MERGE y consultas parametrizadas.\n",
    "        Utiliza transacciones explícitas y maneja la detección de APOC.\n",
    "        Devuelve True si la carga fue exitosa (sin errores), False en caso contrario.\n",
    "        \"\"\"\n",
    "        if not graph_data or (not graph_data.get(\"nodes\") and not graph_data.get(\"relationships\")):\n",
    "             logger.warning(\"No hay datos de grafo válidos para cargar en Neo4j.\")\n",
    "             return True # Éxito si no hay nada que cargar\n",
    "\n",
    "        nodes_to_load = graph_data.get(\"nodes\", [])\n",
    "        rels_to_load = graph_data.get(\"relationships\", [])\n",
    "\n",
    "        logger.info(f\"Iniciando carga a Neo4j: {len(nodes_to_load)} nodos, {len(rels_to_load)} relaciones...\")\n",
    "        start_time = time.time()\n",
    "        nodes_processed_count = 0 # Contador basado en lotes procesados sin error\n",
    "        rels_processed_count = 0  # Contador basado en lotes procesados sin error\n",
    "        errors = 0\n",
    "        apoc_available = False # Flag para saber qué queries usar\n",
    "\n",
    "        # --- Inicio Transacción ---\n",
    "        try:\n",
    "            with driver.session(database=db_name, default_access_mode=\"WRITE\") as session:\n",
    "\n",
    "                # --- Detectar APOC ---\n",
    "                try:\n",
    "                     session.run(\"RETURN apoc.version() LIMIT 1\").consume()\n",
    "                     apoc_available = True\n",
    "                     logger.info(\"APOC detectado. Se usarán funciones APOC para carga.\")\n",
    "                except Exception:\n",
    "                     logger.warning(\"APOC no detectado o no funcional. Se usarán métodos alternativos.\")\n",
    "                     apoc_available = False\n",
    "\n",
    "                # --- Cargar Nodos ---\n",
    "                if nodes_to_load:\n",
    "                    logger.debug(f\"Ejecutando carga de {len(nodes_to_load)} nodos...\")\n",
    "                    if apoc_available:\n",
    "                        node_query = \"\"\"\n",
    "                        UNWIND $nodes AS node_batch\n",
    "                        MERGE (n {id: node_batch.id})\n",
    "                        ON CREATE SET n = node_batch.properties, n.id = node_batch.id\n",
    "                        ON MATCH SET n += node_batch.properties\n",
    "                        WITH n, node_batch.label AS labelStr\n",
    "                        WHERE labelStr IS NOT NULL AND labelStr <> ''\n",
    "                        CALL apoc.create.addLabels(n, [labelStr]) YIELD node\n",
    "                        RETURN count(node) AS processed_count\n",
    "                        \"\"\"\n",
    "                    else: # Sin APOC\n",
    "                        node_query = \"\"\"\n",
    "                        UNWIND $nodes AS node_batch\n",
    "                        MERGE (n {id: node_batch.id})\n",
    "                        ON CREATE SET n = node_batch.properties, n.id = node_batch.id, n._label = node_batch.label\n",
    "                        ON MATCH SET n += node_batch.properties, n._label = node_batch.label\n",
    "                        RETURN count(n) AS processed_count\n",
    "                        \"\"\"\n",
    "                    try:\n",
    "                        batch_size = 500\n",
    "                        for i in range(0, len(nodes_to_load), batch_size):\n",
    "                            batch = nodes_to_load[i:i+batch_size]\n",
    "                            logger.debug(f\"  Cargando lote de nodos {i+1}-{i+len(batch)}...\")\n",
    "                            # --- CORRECCIÓN APLICADA AQUÍ ---\n",
    "                            # Ejecutar y consumir para obtener summary y detectar errores\n",
    "                            summary = session.run(node_query, parameters={\"nodes\": batch}).consume()\n",
    "                            # Registrar notificaciones (advertencias) de Neo4j\n",
    "                            if summary.notifications:\n",
    "                                logger.warning(f\"Notificaciones Neo4j (carga nodos lote {i+1}): {summary.notifications}\")\n",
    "                            # Contar el lote como procesado si no hubo excepción\n",
    "                            nodes_processed_count += len(batch)\n",
    "                            # --------------------------------\n",
    "                        logger.info(f\"Lotes de nodos procesados/intentados en Neo4j: {nodes_processed_count}\")\n",
    "                    except Exception as node_e:\n",
    "                        logger.error(f\"Error durante carga de lote de nodos: {node_e}\", exc_info=True)\n",
    "                        errors += 1\n",
    "                        raise # Relanzar para abortar la transacción completa\n",
    "\n",
    "                else:\n",
    "                    logger.info(\"No hay nodos para cargar.\")\n",
    "\n",
    "                # --- Cargar Relaciones (Solo si no hubo errores en nodos) ---\n",
    "                if rels_to_load and errors == 0:\n",
    "                    logger.debug(f\"Ejecutando carga de {len(rels_to_load)} relaciones...\")\n",
    "                    if apoc_available:\n",
    "                        rel_query = \"\"\"\n",
    "                        UNWIND $rels AS rel_batch\n",
    "                        MATCH (source {id: rel_batch.source})\n",
    "                        MATCH (target {id: rel_batch.target})\n",
    "                        CALL apoc.create.relationship(source, rel_batch.type, rel_batch.properties, target) YIELD rel\n",
    "                        RETURN count(rel) AS processed_count\n",
    "                        \"\"\"\n",
    "                    else: # Sin APOC\n",
    "                        rel_query = \"\"\"\n",
    "                        UNWIND $rels AS rel_batch\n",
    "                        MATCH (source {id: rel_batch.source})\n",
    "                        MATCH (target {id: rel_batch.target})\n",
    "                        MERGE (source)-[r:RELATED]->(target)\n",
    "                        SET r = rel_batch.properties, r.type = rel_batch.type\n",
    "                        RETURN count(r) AS processed_count\n",
    "                        \"\"\"\n",
    "                    try:\n",
    "                        batch_size = 500\n",
    "                        for i in range(0, len(rels_to_load), batch_size):\n",
    "                             batch = rels_to_load[i:i+batch_size]\n",
    "                             logger.debug(f\"  Cargando lote de relaciones {i+1}-{i+len(batch)}...\")\n",
    "                             # --- CORRECCIÓN APLICADA AQUÍ ---\n",
    "                             # Ejecutar y consumir\n",
    "                             summary = session.run(rel_query, parameters={\"rels\": batch}).consume()\n",
    "                             if summary.notifications:\n",
    "                                 logger.warning(f\"Notificaciones Neo4j (carga rels lote {i+1}): {summary.notifications}\")\n",
    "                             # Contar lote como procesado si no hubo excepción\n",
    "                             rels_processed_count += len(batch)\n",
    "                             # --------------------------------\n",
    "                        logger.info(f\"Lotes de relaciones procesados/intentados en Neo4j: {rels_processed_count}\")\n",
    "                    except Exception as rel_e:\n",
    "                         logger.error(f\"Error durante carga de lote de relaciones: {rel_e}\", exc_info=True)\n",
    "                         errors += 1\n",
    "                         raise # Abortar transacción\n",
    "                elif errors > 0:\n",
    "                     logger.warning(\"Carga de relaciones omitida debido a errores previos en carga de nodos.\")\n",
    "                else:\n",
    "                    logger.info(\"No hay relaciones para cargar.\")\n",
    "\n",
    "            # Fin de la transacción (commit automático si no hubo excepciones)\n",
    "            duration = time.time() - start_time\n",
    "            logger.info(f\"Carga a Neo4j (transacción) finalizada en {duration:.2f}s. Errores DENTRO de la transacción: {errors}\")\n",
    "            return errors == 0 # Devuelve True si no hubo errores\n",
    "\n",
    "        except Exception as tx_e: # Capturar error que abortó la transacción\n",
    "             logger.error(f\"Error en la transacción de carga Neo4j (rollback realizado): {tx_e}\", exc_info=True)\n",
    "             return False # La carga falló\n",
    "\n",
    "\n",
    "    # --- Métodos de Consulta (Solo si Neo4j está cargado) ---\n",
    "    def query_graph(self, question: str, query_prompt_template: PromptTemplate) -> str:\n",
    "        \"\"\"Consulta el grafo Neo4j usando el LLM principal.\"\"\"\n",
    "        global driver # Acceder al driver global si está disponible para resumen\n",
    "        if not self.graph: return \"Error: Neo4j no configurado para consultas en este procesador.\"\n",
    "        if not query_prompt_template: return \"Error: Falta plantilla de consulta.\"\n",
    "        if not self.llm: return \"Error: LLM principal no inicializado.\"\n",
    "\n",
    "        logger.info(f\"Recibida consulta al grafo Neo4j: '{question}'\")\n",
    "        try:\n",
    "            # Obtener esquema y resumen actualizados de Neo4j\n",
    "            logger.debug(\"Refrescando esquema Neo4j para consulta...\")\n",
    "            self.graph.refresh_schema()\n",
    "            schema_context = self.graph.schema\n",
    "            logger.debug(f\"Esquema Neo4j para contexto:\\n{schema_context}\")\n",
    "\n",
    "            summary_context = \"(Resumen Neo4j no disponible)\"\n",
    "            db_to_query = self.graph.database # Obtener DB del wrapper\n",
    "            if driver and driver.is_open():\n",
    "                 logger.debug(f\"Intentando obtener resumen con driver directo para DB: {db_to_query}\")\n",
    "                 summary_context = retrieve_graph_summary(driver, db_name=db_to_query)\n",
    "            else:\n",
    "                 logger.debug(\"Driver directo no disponible, intentando resumen simple vía wrapper...\")\n",
    "                 try: # Fallback con consulta simple vía wrapper\n",
    "                     summary_res = self.graph.query(\"CALL db.labels() YIELD label RETURN collect(label) as labels\")\n",
    "                     summary_context = f\"Tipos de Nodos: {summary_res[0]['labels'] if summary_res else 'N/A'}\"\n",
    "                 except Exception as e_sum: logger.warning(f\"Fallo resumen simple vía wrapper: {e_sum}\")\n",
    "\n",
    "            logger.info(f\"Invocando LLM principal ({type(self.llm)}) para consulta Neo4j...\")\n",
    "            query_chain = query_prompt_template | self.llm | StrOutputParser()\n",
    "            start_time = time.time()\n",
    "            response = query_chain.invoke({\n",
    "                \"question\": question,\n",
    "                \"knowledge_graph_schema\": schema_context,\n",
    "                \"knowledge_graph_summary\": summary_context\n",
    "            })\n",
    "            duration = time.time() - start_time\n",
    "            logger.info(f\"Respuesta consulta Neo4j ({type(self.llm)}) recibida en {duration:.2f}s.\")\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error durante consulta del grafo Neo4j: {e}\", exc_info=True)\n",
    "            return \"Error: Problema al procesar la consulta a Neo4j.\"\n",
    "\n",
    "\n",
    "    def interactive_query(self, query_prompt_template: PromptTemplate):\n",
    "        \"\"\"Inicia bucle de consulta interactiva contra Neo4j.\"\"\"\n",
    "        if not self.graph: print(\"Error: Neo4j no configurado.\"); return\n",
    "        if not self.llm: print(\"Error: LLM principal no listo.\"); return\n",
    "        if not query_prompt_template: print(\"Error: Falta plantilla de consulta.\"); return\n",
    "\n",
    "        print(\"\\n--- Modo Consulta Interactiva (Neo4j) ---\")\n",
    "        print(f\"(Usando LLM principal: {llm_type})\")\n",
    "        print(\"Introduce tu pregunta sobre el grafo Neo4j. Escribe 'exit' o 'quit' para salir.\")\n",
    "\n",
    "        while True:\n",
    "            try: question = input(\"\\nPregunta Neo4j> \").strip()\n",
    "            except EOFError: logger.warning(\"EOF recibido, saliendo.\"); break\n",
    "            if question.lower() in ['exit', 'quit']: logger.info(\"Saliendo modo interactivo.\"); break\n",
    "            if not question: continue\n",
    "\n",
    "            print(\"Procesando consulta Neo4j...\")\n",
    "            answer = self.query_graph(question, query_prompt_template)\n",
    "            print(\"\\nRespuesta:\") ; print(\"-\" * 10) ; print(answer) ; print(\"-\" * 10)\n",
    "        print(\"--- Fin Modo Interactivo ---\")\n",
    "\n",
    "\n",
    "print(\"Clase GraphProcessor definida.\")\n",
    "logger.info(\"Bloque 7: Definición de GraphProcessor completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ec7b71a-ecf9-4d1a-9a4c-218d866e741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:48:17,814 - INFO - [__main__:4] - Bloque 8: Configurando conexión Neo4j (si es necesario)...\n",
      "2025-04-27 15:48:17,816 - INFO - [__main__:33] - Verificación inicial de variables globales de configuración pasada.\n",
      "2025-04-27 15:48:17,817 - INFO - [__main__:56] - Se requiere conexión Neo4j (para carga o consulta).\n",
      "2025-04-27 15:48:17,817 - INFO - [__main__:67] - Obteniendo driver directo Neo4j...\n",
      "2025-04-27 15:48:17,821 - INFO - [__main__:33] - Conexión directa con Neo4j en bolt://TIKA_Neo4j_02:7687 verificada (Server address: 172.24.0.4:7687, Protocol: 5.4).\n",
      "2025-04-27 15:48:17,822 - INFO - [__main__:74] - Driver Neo4j conectado a BD: 'default'\n",
      "2025-04-27 15:48:17,822 - INFO - [__main__:78] - Inicializando wrapper Neo4jGraph para BD 'default' (para consultas)...\n",
      "2025-04-27 15:48:17,984 - INFO - [__main__:82] - Wrapper Neo4jGraph conectado a BD 'default'.\n",
      "2025-04-27 15:48:17,985 - WARNING - [__main__:95] - 'delete_indexes_before_load' activado para BD 'default'.\n",
      "2025-04-27 15:48:17,985 - WARNING - [__main__:192] - Intentando eliminar índices de usuario en BD 'default'...\n",
      "2025-04-27 15:48:18,015 - INFO - [__main__:215] - Se encontraron 0 índices de usuario para posible borrado.\n",
      "2025-04-27 15:48:18,016 - INFO - [__main__:235] - No se encontraron índices de usuario para eliminar en BD 'default'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "=== Bloque 8: Configuración Opcional Neo4j ===\n",
      "==================================================\n",
      "Bloque 8: Verificando existencia de variables globales...\n",
      "Bloque 8: Estado GLOBAL de 'load_into_neo4j': True\n",
      "Bloque 8: Estado GLOBAL de 'run_interactive_query': True\n",
      "Wrapper Neo4jGraph listo para consultas en BD 'default'.\n",
      "\n",
      "Intentando borrar índices usuario en BD 'default'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:48:18,017 - WARNING - [__main__:100] - 'clear_graph_before_load' activado para BD 'default'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hay índices de usuario para borrar en 'default'.\n",
      "--------------------\n",
      "\n",
      "Intentando borrar TODOS los datos en BD 'default'...\n",
      "\n",
      "ADVERTENCIA MUY SERIA:\n",
      "Estás a punto de BORRAR **TODOS** los nodos y relaciones de la base de datos 'neo4j'.\n",
      "Esta acción es IRREVERSIBLE.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Escribe 'SI QUIERO BORRAR TODO' para continuar con la segunda confirmación:  SI QUIERO BORRAR TODO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SEGUNDA CONFIRMACIÓN (IRREVERSIBLE):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Escribe 'BORRAR TODO NEO4J AHORA' para proceder con el borrado de 'neo4j':  BORRAR TODO NEO4J AHORA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:49:03,678 - WARNING - [__main__:69] - CONFIRMACIÓN DOBLE RECIBIDA. PROCEDIENDO CON BORRADO COMPLETO en BD 'neo4j'...\n",
      "2025-04-27 15:49:03,679 - INFO - [__main__:73] - Ejecutando 'MATCH (n) DETACH DELETE n'...\n",
      "2025-04-27 15:49:03,690 - INFO - [__main__:81] - Datos del grafo reseteados en BD 'neo4j' en 0.01s. Nodos borrados: 0, Relaciones borradas: 0.\n",
      "2025-04-27 15:49:03,691 - INFO - [__main__:128] - Bloque 8: Configuración opcional Neo4j completada.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos del grafo en 'neo4j' borrados.\n",
      "--------------------\n",
      "Configuración Neo4j completada (Driver y Wrapper).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# %% Bloque 8: Configuración Opcional de Neo4j\n",
    "# ============================================================\n",
    "logger.info(\"Bloque 8: Configurando conexión Neo4j (si es necesario)...\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n=== Bloque 8: Configuración Opcional Neo4j ===\\n\" + \"=\"*50)\n",
    "\n",
    "# --- Verificación INICIAL de variables del Bloque 2 (usando globals()) ---\n",
    "# Comprueba si las variables existen en el scope GLOBAL del kernel\n",
    "print(\"Bloque 8: Verificando existencia de variables globales...\") # DEBUG PRINT\n",
    "required_vars = ['load_into_neo4j', 'run_interactive_query', 'neo4j_configured',\n",
    "                 'NEO4J_URI', 'NEO4J_USERNAME', 'NEO4J_PASSWORD', 'NEO4J_DATABASE',\n",
    "                 'delete_indexes_before_load', 'clear_graph_before_load',\n",
    "                 'show_neo4j_browser'] # Añadida show_neo4j_browser\n",
    "\n",
    "# Comprobar usando globals()\n",
    "missing_vars = [var for var in required_vars if var not in globals()]\n",
    "\n",
    "# Imprimir estado de variables clave para depuración\n",
    "try:\n",
    "    print(f\"Bloque 8: Estado GLOBAL de 'load_into_neo4j': {globals().get('load_into_neo4j', 'NO ENCONTRADA')}\")\n",
    "    print(f\"Bloque 8: Estado GLOBAL de 'run_interactive_query': {globals().get('run_interactive_query', 'NO ENCONTRADA')}\")\n",
    "except Exception as e_dbg:\n",
    "    print(f\"Bloque 8: Error al verificar variables globales: {e_dbg}\")\n",
    "\n",
    "\n",
    "if missing_vars:\n",
    "    error_msg = f\"Faltan variables GLOBALES de configuración (¿Bloque 2 no ejecutado correctamente en este kernel?): {', '.join(missing_vars)}.\"\n",
    "    logger.error(error_msg)\n",
    "    print(f\"ERROR: {error_msg}\")\n",
    "    # Detener la ejecución de esta celda si faltan variables clave\n",
    "    raise NameError(f\"Variables globales requeridas no definidas: {', '.join(missing_vars)}\")\n",
    "else:\n",
    "    logger.info(\"Verificación inicial de variables globales de configuración pasada.\")\n",
    "    # --- Asignar variables globales a locales para este bloque ---\n",
    "    # Esto asegura que el resto del código del bloque use los valores correctos.\n",
    "    load_into_neo4j = globals()['load_into_neo4j']\n",
    "    run_interactive_query = globals()['run_interactive_query']\n",
    "    neo4j_configured = globals()['neo4j_configured']\n",
    "    NEO4J_URI = globals()['NEO4J_URI']\n",
    "    NEO4J_USERNAME = globals()['NEO4J_USERNAME']\n",
    "    NEO4J_PASSWORD = globals()['NEO4J_PASSWORD']\n",
    "    NEO4J_DATABASE = globals()['NEO4J_DATABASE']\n",
    "    delete_indexes_before_load = globals()['delete_indexes_before_load']\n",
    "    clear_graph_before_load = globals()['clear_graph_before_load']\n",
    "    show_neo4j_browser = globals()['show_neo4j_browser']\n",
    "    # --------------------------------------------------------------\n",
    "\n",
    "# --- Variables para este bloque ---\n",
    "# Estas SÍ son locales a este bloque/celda\n",
    "driver: Optional[Driver] = None\n",
    "graph: Optional[Neo4jGraph] = None # Wrapper Langchain\n",
    "\n",
    "# --- Lógica de conexión ---\n",
    "# Ahora usa las variables locales definidas justo arriba\n",
    "if load_into_neo4j or run_interactive_query:\n",
    "    logger.info(\"Se requiere conexión Neo4j (para carga o consulta).\")\n",
    "    if not neo4j_configured:\n",
    "        logger.error(\"Falta NEO4J_PASSWORD. No se puede conectar a Neo4j.\")\n",
    "        print(\"ERROR: Falta contraseña Neo4j. La carga/consulta fallará.\")\n",
    "        # Desactivar flags locales para evitar errores posteriores en esta ejecución\n",
    "        load_into_neo4j = False\n",
    "        run_interactive_query = False\n",
    "        show_neo4j_browser = False\n",
    "    else:\n",
    "        try:\n",
    "            # 1. Driver Directo (para limpieza opcional y carga robusta)\n",
    "            logger.info(\"Obteniendo driver directo Neo4j...\")\n",
    "            # Usar la función definida en Bloque 5 (debe estar en globals())\n",
    "            if 'get_neo4j_driver' not in globals(): raise NameError(\"Función get_neo4j_driver no definida. Ejecuta Bloque 5.\")\n",
    "            driver = get_neo4j_driver(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "            if not driver: raise ConnectionError(\"Fallo al obtener driver directo Neo4j.\")\n",
    "\n",
    "            db_name_for_log = NEO4J_DATABASE if NEO4J_DATABASE else \"default\"\n",
    "            logger.info(f\"Driver Neo4j conectado a BD: '{db_name_for_log}'\")\n",
    "\n",
    "            # 2. Wrapper Langchain (SOLO si se va a consultar interactivamente)\n",
    "            if run_interactive_query: # Usa la variable local\n",
    "                logger.info(f\"Inicializando wrapper Neo4jGraph para BD '{db_name_for_log}' (para consultas)...\")\n",
    "                if 'Neo4jGraph' not in globals(): raise NameError(\"Clase Neo4jGraph no importada. Ejecuta Bloque 1.\")\n",
    "                graph = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE)\n",
    "                graph.refresh_schema() # Verificar conexión\n",
    "                logger.info(f\"Wrapper Neo4jGraph conectado a BD '{db_name_for_log}'.\")\n",
    "                print(f\"Wrapper Neo4jGraph listo para consultas en BD '{db_name_for_log}'.\")\n",
    "            else:\n",
    "                 logger.info(\"Wrapper Neo4jGraph no necesario (run_interactive_query=False).\")\n",
    "                 graph = None # Asegurar que sea None si no se usa\n",
    "\n",
    "            # 3. Limpieza Opcional (ANTES de cualquier carga, requiere driver)\n",
    "            if load_into_neo4j: # Usa la variable local\n",
    "                 # Asegurarse que las funciones de limpieza están definidas\n",
    "                 if 'delete_all_user_indexes' not in globals() or 'reset_graph_data' not in globals():\n",
    "                      raise NameError(\"Funciones de limpieza (delete_all_user_indexes/reset_graph_data) no definidas. Ejecuta Bloque 5.\")\n",
    "\n",
    "                 if delete_indexes_before_load: # Usa la variable local\n",
    "                     logger.warning(f\"'delete_indexes_before_load' activado para BD '{db_name_for_log}'.\")\n",
    "                     print(f\"\\nIntentando borrar índices usuario en BD '{db_name_for_log}'...\")\n",
    "                     delete_all_user_indexes(driver, db_name=NEO4J_DATABASE)\n",
    "                     print(\"-\" * 20) # Separador visual\n",
    "                 if clear_graph_before_load: # Usa la variable local\n",
    "                     logger.warning(f\"'clear_graph_before_load' activado para BD '{db_name_for_log}'.\")\n",
    "                     print(f\"\\nIntentando borrar TODOS los datos en BD '{db_name_for_log}'...\")\n",
    "                     # La función reset_graph_data ya pide doble confirmación\n",
    "                     reset_graph_data(driver, db_name=NEO4J_DATABASE)\n",
    "                     print(\"-\" * 20) # Separador visual\n",
    "\n",
    "            print(f\"Configuración Neo4j completada (Driver {'y Wrapper' if graph else 'solamente'}).\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error crítico durante configuración Neo4j: {e}\", exc_info=True)\n",
    "            print(f\"ERROR configurando Neo4j: {e}\")\n",
    "            # Limpieza de driver\n",
    "            if driver:\n",
    "                try:\n",
    "                    driver.close()\n",
    "                    logger.info(\"Driver cerrado debido a error en configuración.\")\n",
    "                except Exception as close_e:\n",
    "                    logger.error(f\"Error al intentar cerrar el driver tras error: {close_e}\")\n",
    "            driver = None; graph = None\n",
    "            # Desactivar flags locales que dependen de Neo4j si falla la conexión\n",
    "            load_into_neo4j = False\n",
    "            run_interactive_query = False\n",
    "            show_neo4j_browser = False\n",
    "            logger.warning(\"Se desactivó la carga y consulta a Neo4j debido a error de conexión/configuración.\")\n",
    "else:\n",
    "    logger.info(\"No se requiere conexión Neo4j para esta ejecución (load_into_neo4j=False y run_interactive_query=False).\")\n",
    "    print(\"Conexión Neo4j no necesaria.\")\n",
    "\n",
    "logger.info(\"Bloque 8: Configuración opcional Neo4j completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8385a0e6-fdd4-4f7b-8db9-c30e6a980c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:49:10,499 - INFO - [__main__:4] - Bloque 9: Iniciando fase de extracción de grafo a JSON...\n",
      "2025-04-27 15:49:10,501 - INFO - [__main__:20] - Instanciando GraphProcessor...\n",
      "2025-04-27 15:49:10,502 - INFO - [__main__:31] - GraphProcessor inicializado con LLM principal tipo: <class 'langchain_groq.chat_models.ChatGroq'>\n",
      "2025-04-27 15:49:10,502 - INFO - [__main__:34] - Instancia Neo4jGraph proporcionada (para carga/consulta opcional).\n",
      "2025-04-27 15:49:10,502 - INFO - [__main__:43] - Configurando LLM Ollama para chunking (Modelo: gemma3:27b)...\n",
      "/tmp/ipykernel_73752/2274387750.py:45: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm_chunking = ChatOllama(model=ollama_chunking_model, base_url=ollama_url, format=\"json\", temperature=0.0, request_timeout=120.0) # Timeout 2 min\n",
      "2025-04-27 15:49:10,505 - INFO - [__main__:54] - Cadena de Chunking (Ollama JSON + Fixer) configurada.\n",
      "2025-04-27 15:49:10,505 - INFO - [__main__:62] - Configurando cadena de extracción de grafo con LLM principal (<class 'langchain_groq.chat_models.ChatGroq'>)...\n",
      "2025-04-27 15:49:10,507 - INFO - [__main__:78] - Cadena de Extracción de Grafo (LLM Principal + Pydantic Parser) configurada.\n",
      "2025-04-27 15:49:10,508 - INFO - [__main__:32] - GraphProcessor instanciado.\n",
      "2025-04-27 15:49:10,508 - INFO - [__main__:43] - --- Procesando archivo 1/2: Conciencia.md ---\n",
      "2025-04-27 15:49:10,509 - INFO - [__main__:343] - --- Iniciando procesamiento del archivo: datasets/Conciencia.md ---\n",
      "2025-04-27 15:49:10,510 - INFO - [__main__:103] - Archivo leído: datasets/Conciencia.md (codificación: utf-8)\n",
      "2025-04-27 15:49:10,510 - INFO - [__main__:129] - Iniciando división de texto con Ollama (modo JSON)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "=== Bloque 9: Extracción de Grafo a JSON ===\n",
      "==================================================\n",
      "GraphProcessor listo para extracción.\n",
      "\n",
      "Procesando archivo 1/2: Conciencia.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 16:05:53,687 - INFO - [__main__:139] - Chunking Ollama completado (23 chunks) en 1003.18s.\n",
      "2025-04-27 16:05:53,687 - INFO - [__main__:373] - Iniciando extracción de grafo para 23 chunks de Conciencia.md...\n",
      "2025-04-27 16:05:53,688 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 1 (len=98 chars)...\n",
      "2025-04-27 16:05:54,852 - INFO - [__main__:186] - Extracción grafo chunk 1 completada (1.16s). Nodos: 4, Rels: 3.\n",
      "2025-04-27 16:05:54,852 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 2 (len=155 chars)...\n",
      "2025-04-27 16:05:55,617 - INFO - [__main__:186] - Extracción grafo chunk 2 completada (0.76s). Nodos: 2, Rels: 1.\n",
      "2025-04-27 16:05:55,618 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 3 (len=189 chars)...\n",
      "2025-04-27 16:05:56,332 - INFO - [__main__:186] - Extracción grafo chunk 3 completada (0.71s). Nodos: 3, Rels: 2.\n",
      "2025-04-27 16:05:56,332 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 4 (len=194 chars)...\n",
      "2025-04-27 16:05:57,093 - INFO - [__main__:186] - Extracción grafo chunk 4 completada (0.76s). Nodos: 2, Rels: 1.\n",
      "2025-04-27 16:05:57,094 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 5 (len=150 chars)...\n",
      "2025-04-27 16:05:57,840 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 15.000000 seconds\n",
      "2025-04-27 16:06:13,741 - INFO - [__main__:186] - Extracción grafo chunk 5 completada (16.65s). Nodos: 2, Rels: 1.\n",
      "2025-04-27 16:06:13,742 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 6 (len=150 chars)...\n",
      "2025-04-27 16:06:13,824 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 13.000000 seconds\n",
      "2025-04-27 16:06:27,752 - INFO - [__main__:186] - Extracción grafo chunk 6 completada (14.01s). Nodos: 4, Rels: 3.\n",
      "2025-04-27 16:06:27,753 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 7 (len=139 chars)...\n",
      "2025-04-27 16:06:27,845 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 16.000000 seconds\n",
      "2025-04-27 16:06:44,687 - INFO - [__main__:186] - Extracción grafo chunk 7 completada (16.93s). Nodos: 2, Rels: 1.\n",
      "2025-04-27 16:06:44,688 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 8 (len=80 chars)...\n",
      "2025-04-27 16:06:44,767 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "2025-04-27 16:06:59,529 - INFO - [__main__:186] - Extracción grafo chunk 8 completada (14.84s). Nodos: 2, Rels: 1.\n",
      "2025-04-27 16:06:59,529 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 9 (len=166 chars)...\n",
      "2025-04-27 16:07:00,117 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 15.000000 seconds\n",
      "2025-04-27 16:07:16,006 - INFO - [__main__:186] - Extracción grafo chunk 9 completada (16.48s). Nodos: 4, Rels: 3.\n",
      "2025-04-27 16:07:16,007 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 10 (len=106 chars)...\n",
      "2025-04-27 16:07:16,088 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 15.000000 seconds\n",
      "2025-04-27 16:07:31,967 - INFO - [__main__:186] - Extracción grafo chunk 10 completada (15.96s). Nodos: 3, Rels: 2.\n",
      "2025-04-27 16:07:31,968 - INFO - [__main__:382] -   Progreso extracción Conciencia.md: Chunk 11/23... (98.3s desde último log)\n",
      "2025-04-27 16:07:31,968 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 11 (len=88 chars)...\n",
      "2025-04-27 16:07:32,058 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 16.000000 seconds\n",
      "2025-04-27 16:07:48,994 - INFO - [__main__:186] - Extracción grafo chunk 11 completada (17.03s). Nodos: 2, Rels: 1.\n",
      "2025-04-27 16:07:48,995 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 12 (len=88 chars)...\n",
      "2025-04-27 16:07:49,085 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "2025-04-27 16:08:03,907 - INFO - [__main__:186] - Extracción grafo chunk 12 completada (14.91s). Nodos: 2, Rels: 1.\n",
      "2025-04-27 16:08:03,907 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 13 (len=125 chars)...\n",
      "2025-04-27 16:08:04,011 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 15.000000 seconds\n",
      "2025-04-27 16:08:19,908 - INFO - [__main__:186] - Extracción grafo chunk 13 completada (16.00s). Nodos: 5, Rels: 4.\n",
      "2025-04-27 16:08:19,908 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 14 (len=128 chars)...\n",
      "2025-04-27 16:08:19,992 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 16.000000 seconds\n",
      "2025-04-27 16:08:37,619 - INFO - [__main__:186] - Extracción grafo chunk 14 completada (17.71s). Nodos: 3, Rels: 2.\n",
      "2025-04-27 16:08:37,619 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 15 (len=117 chars)...\n",
      "2025-04-27 16:08:37,700 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "2025-04-27 16:08:52,763 - INFO - [__main__:186] - Extracción grafo chunk 15 completada (15.14s). Nodos: 3, Rels: 2.\n",
      "2025-04-27 16:08:52,764 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 16 (len=127 chars)...\n",
      "2025-04-27 16:08:52,846 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 15.000000 seconds\n",
      "2025-04-27 16:09:08,965 - INFO - [__main__:186] - Extracción grafo chunk 16 completada (16.20s). Nodos: 3, Rels: 2.\n",
      "2025-04-27 16:09:08,966 - INFO - [__main__:382] -   Progreso extracción Conciencia.md: Chunk 17/23... (97.0s desde último log)\n",
      "2025-04-27 16:09:08,966 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 17 (len=108 chars)...\n",
      "2025-04-27 16:09:09,079 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 15.000000 seconds\n",
      "2025-04-27 16:09:25,419 - INFO - [__main__:186] - Extracción grafo chunk 17 completada (16.45s). Nodos: 3, Rels: 2.\n",
      "2025-04-27 16:09:25,420 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 18 (len=131 chars)...\n",
      "2025-04-27 16:09:25,597 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "2025-04-27 16:09:40,535 - INFO - [__main__:186] - Extracción grafo chunk 18 completada (15.12s). Nodos: 2, Rels: 1.\n",
      "2025-04-27 16:09:40,536 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 19 (len=131 chars)...\n",
      "2025-04-27 16:09:40,774 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 15.000000 seconds\n",
      "2025-04-27 16:09:56,754 - INFO - [__main__:186] - Extracción grafo chunk 19 completada (16.22s). Nodos: 2, Rels: 1.\n",
      "2025-04-27 16:09:56,755 - INFO - [__main__:382] -   Progreso extracción Conciencia.md: Chunk 20/23... (47.8s desde último log)\n",
      "2025-04-27 16:09:56,755 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 20 (len=123 chars)...\n",
      "2025-04-27 16:09:56,845 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "2025-04-27 16:10:11,731 - INFO - [__main__:186] - Extracción grafo chunk 20 completada (14.98s). Nodos: 3, Rels: 2.\n",
      "2025-04-27 16:10:11,731 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 21 (len=133 chars)...\n",
      "2025-04-27 16:10:11,808 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 16.000000 seconds\n",
      "2025-04-27 16:10:28,635 - INFO - [__main__:186] - Extracción grafo chunk 21 completada (16.90s). Nodos: 2, Rels: 1.\n",
      "2025-04-27 16:10:28,636 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 22 (len=139 chars)...\n",
      "2025-04-27 16:10:28,721 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "2025-04-27 16:10:43,578 - INFO - [__main__:186] - Extracción grafo chunk 22 completada (14.94s). Nodos: 4, Rels: 3.\n",
      "2025-04-27 16:10:43,579 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 23 (len=148 chars)...\n",
      "2025-04-27 16:10:43,671 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 16.000000 seconds\n",
      "2025-04-27 16:11:00,564 - INFO - [__main__:186] - Extracción grafo chunk 23 completada (16.98s). Nodos: 3, Rels: 2.\n",
      "2025-04-27 16:11:00,565 - INFO - [__main__:399] - Extracción de chunks para Conciencia.md completada. Exitosos/Intentados: 23/23, Fallos: 0.\n",
      "2025-04-27 16:11:00,565 - INFO - [__main__:415] - Agregando resultados de 23 partes extraídas para Conciencia.md...\n",
      "/tmp/ipykernel_73752/2274387750.py:281: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  \"nodes\": [node.dict() for node in aggregated_nodes.values()],\n",
      "/tmp/ipykernel_73752/2274387750.py:282: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  \"relationships\": [rel.dict() for rel in aggregated_relationships_list]\n",
      "2025-04-27 16:11:00,566 - INFO - [__main__:284] - Agregación completada. Nodos únicos: 33, Relaciones únicas: 40.\n",
      "2025-04-27 16:11:00,566 - INFO - [__main__:290] - Intentando guardar JSON en: output_graphs/Conciencia.graph.json\n",
      "2025-04-27 16:11:00,568 - INFO - [__main__:296] - Estructura del grafo guardada exitosamente.\n",
      "2025-04-27 16:11:00,568 - INFO - [__main__:427] - --- Procesamiento JSON para archivo Conciencia.md completado ---\n",
      "2025-04-27 16:11:00,569 - INFO - [__main__:43] - --- Procesando archivo 2/2: Microsoft.md ---\n",
      "2025-04-27 16:11:00,569 - INFO - [__main__:343] - --- Iniciando procesamiento del archivo: datasets/Microsoft.md ---\n",
      "2025-04-27 16:11:00,570 - INFO - [__main__:103] - Archivo leído: datasets/Microsoft.md (codificación: utf-8)\n",
      "2025-04-27 16:11:00,571 - INFO - [__main__:129] - Iniciando división de texto con Ollama (modo JSON)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Visualización Previa JSON Extraído ---\n",
      "{\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"id\": \"Consciousness\",\n",
      "      \"label\": \"Concept\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"Humans\",\n",
      "      \"label\": \"Group\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"Animals\",\n",
      "      \"label\": \"Group\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"Machines\",\n",
      "      \"label\": \"Group\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"Human\",\n",
      "      \"label\": \"Person\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"Auto-Report\",\n",
      "      \"label\": \"Concept\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"NonHumanAnimals\",\n",
      "      \"label\": \"Concept\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"Species\",\n",
      "      \"label\": \"Concept\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ArtificialIntelligence\",\n",
      "      \"label\": \"Concept\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"Conciencia\",\n",
      "      \"label\": \"Concept\",\n",
      "      \"properties\": {}\n",
      "    }\n",
      "  ],\n",
      "  \"relationships\": [\n",
      "    {\n",
      "      \"source\": \"Consciousness\",\n",
      "      \"target\": \"Humans\",\n",
      "      \"type\": \"RELATED_TO\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Consciousness\",\n",
      "      \"target\": \"Animals\",\n",
      "      \"type\": \"RELATED_TO\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Consciousness\",\n",
      "      \"target\": \"Machines\",\n",
      "      \"type\": \"RELATED_TO\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Human\",\n",
      "      \"target\": \"Auto-Report\",\n",
      "      \"type\": \"MENTIONS\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Animals\",\n",
      "      \"target\": \"NonHumanAnimals\",\n",
      "      \"type\": \"SUBCLASS_OF\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"NonHumanAnimals\",\n",
      "      \"target\": \"Species\",\n",
      "      \"type\": \"INSTANCE_OF\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Machines\",\n",
      "      \"target\": \"ArtificialIntelligence\",\n",
      "      \"type\": \"RELATED_TO\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Humans\",\n",
      "      \"target\": \"Consciousness\",\n",
      "      \"type\": \"EXHIBITS\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"AnimalesNoHumanos\",\n",
      "      \"target\": \"Conciencia\",\n",
      "      \"type\": \"RELATED_TO\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"AnimalesNoHumanos\",\n",
      "      \"target\": \"Problema\",\n",
      "      \"type\": \"MENTIONS\",\n",
      "      \"properties\": {}\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "  (... 23 nodos más ...)\n",
      "  (... 30 relaciones más ...)\n",
      "--- (Mostrando hasta 10 nodos/relaciones. Total: 33 N, 40 R) ---\n",
      "-> Procesado (1310.06s). Nodos: 33, Rels: 40. JSON guardado.\n",
      "\n",
      "Procesando archivo 2/2: Microsoft.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 16:27:05,755 - INFO - [__main__:139] - Chunking Ollama completado (22 chunks) en 965.18s.\n",
      "2025-04-27 16:27:05,755 - INFO - [__main__:373] - Iniciando extracción de grafo para 22 chunks de Microsoft.md...\n",
      "2025-04-27 16:27:05,756 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 1 (len=199 chars)...\n",
      "2025-04-27 16:27:06,790 - INFO - [__main__:186] - Extracción grafo chunk 1 completada (1.03s). Nodos: 3, Rels: 2.\n",
      "2025-04-27 16:27:06,791 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 2 (len=86 chars)...\n",
      "2025-04-27 16:27:07,534 - INFO - [__main__:186] - Extracción grafo chunk 2 completada (0.74s). Nodos: 3, Rels: 0.\n",
      "2025-04-27 16:27:07,535 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 3 (len=137 chars)...\n",
      "2025-04-27 16:27:08,376 - INFO - [__main__:186] - Extracción grafo chunk 3 completada (0.84s). Nodos: 4, Rels: 3.\n",
      "2025-04-27 16:27:08,377 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 4 (len=120 chars)...\n",
      "2025-04-27 16:27:09,269 - INFO - [__main__:186] - Extracción grafo chunk 4 completada (0.89s). Nodos: 2, Rels: 1.\n",
      "2025-04-27 16:27:09,269 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 5 (len=120 chars)...\n",
      "2025-04-27 16:27:09,351 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "2025-04-27 16:27:24,415 - INFO - [__main__:186] - Extracción grafo chunk 5 completada (15.15s). Nodos: 3, Rels: 2.\n",
      "2025-04-27 16:27:24,416 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 6 (len=109 chars)...\n",
      "2025-04-27 16:27:24,502 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 15.000000 seconds\n",
      "2025-04-27 16:27:40,419 - INFO - [__main__:186] - Extracción grafo chunk 6 completada (16.00s). Nodos: 2, Rels: 1.\n",
      "2025-04-27 16:27:40,420 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 7 (len=107 chars)...\n",
      "2025-04-27 16:27:40,579 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 15.000000 seconds\n",
      "2025-04-27 16:27:56,384 - INFO - [__main__:186] - Extracción grafo chunk 7 completada (15.96s). Nodos: 2, Rels: 1.\n",
      "2025-04-27 16:27:56,385 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 8 (len=102 chars)...\n",
      "2025-04-27 16:27:56,476 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "2025-04-27 16:28:11,551 - INFO - [__main__:186] - Extracción grafo chunk 8 completada (15.17s). Nodos: 2, Rels: 1.\n",
      "2025-04-27 16:28:11,552 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 9 (len=113 chars)...\n",
      "2025-04-27 16:28:11,719 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 15.000000 seconds\n",
      "2025-04-27 16:28:27,714 - INFO - [__main__:186] - Extracción grafo chunk 9 completada (16.16s). Nodos: 4, Rels: 2.\n",
      "2025-04-27 16:28:27,715 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 10 (len=201 chars)...\n",
      "2025-04-27 16:28:27,797 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 17.000000 seconds\n",
      "2025-04-27 16:28:45,774 - INFO - [__main__:186] - Extracción grafo chunk 10 completada (18.06s). Nodos: 7, Rels: 6.\n",
      "2025-04-27 16:28:45,775 - INFO - [__main__:382] -   Progreso extracción Microsoft.md: Chunk 11/22... (100.0s desde último log)\n",
      "2025-04-27 16:28:45,775 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 11 (len=165 chars)...\n",
      "2025-04-27 16:28:45,904 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 16.000000 seconds\n",
      "2025-04-27 16:29:02,834 - INFO - [__main__:186] - Extracción grafo chunk 11 completada (17.06s). Nodos: 5, Rels: 4.\n",
      "2025-04-27 16:29:02,834 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 12 (len=89 chars)...\n",
      "2025-04-27 16:29:02,911 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 16.000000 seconds\n",
      "2025-04-27 16:29:19,618 - INFO - [__main__:186] - Extracción grafo chunk 12 completada (16.78s). Nodos: 3, Rels: 2.\n",
      "2025-04-27 16:29:19,619 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 13 (len=103 chars)...\n",
      "2025-04-27 16:29:19,717 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 15.000000 seconds\n",
      "2025-04-27 16:29:35,583 - INFO - [__main__:186] - Extracción grafo chunk 13 completada (15.96s). Nodos: 3, Rels: 2.\n",
      "2025-04-27 16:29:35,584 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 14 (len=131 chars)...\n",
      "2025-04-27 16:29:35,690 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 15.000000 seconds\n",
      "2025-04-27 16:29:51,466 - INFO - [__main__:186] - Extracción grafo chunk 14 completada (15.88s). Nodos: 3, Rels: 2.\n",
      "2025-04-27 16:29:51,467 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 15 (len=89 chars)...\n",
      "2025-04-27 16:29:51,651 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "2025-04-27 16:30:07,444 - INFO - [__main__:186] - Extracción grafo chunk 15 completada (15.98s). Nodos: 2, Rels: 1.\n",
      "2025-04-27 16:30:07,445 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 16 (len=84 chars)...\n",
      "2025-04-27 16:30:07,558 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "2025-04-27 16:30:22,495 - INFO - [__main__:186] - Extracción grafo chunk 16 completada (15.05s). Nodos: 3, Rels: 2.\n",
      "2025-04-27 16:30:22,496 - INFO - [__main__:382] -   Progreso extracción Microsoft.md: Chunk 17/22... (96.7s desde último log)\n",
      "2025-04-27 16:30:22,497 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 17 (len=162 chars)...\n",
      "2025-04-27 16:30:22,568 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "2025-04-27 16:30:37,513 - INFO - [__main__:186] - Extracción grafo chunk 17 completada (15.02s). Nodos: 3, Rels: 2.\n",
      "2025-04-27 16:30:37,514 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 18 (len=124 chars)...\n",
      "2025-04-27 16:30:37,651 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 15.000000 seconds\n",
      "2025-04-27 16:30:53,537 - INFO - [__main__:186] - Extracción grafo chunk 18 completada (16.02s). Nodos: 4, Rels: 3.\n",
      "2025-04-27 16:30:53,537 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 19 (len=148 chars)...\n",
      "2025-04-27 16:30:53,688 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 15.000000 seconds\n",
      "2025-04-27 16:31:09,699 - INFO - [__main__:186] - Extracción grafo chunk 19 completada (16.16s). Nodos: 3, Rels: 1.\n",
      "2025-04-27 16:31:09,699 - INFO - [__main__:382] -   Progreso extracción Microsoft.md: Chunk 20/22... (47.2s desde último log)\n",
      "2025-04-27 16:31:09,700 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 20 (len=145 chars)...\n",
      "2025-04-27 16:31:09,786 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 16.000000 seconds\n",
      "2025-04-27 16:31:26,594 - INFO - [__main__:186] - Extracción grafo chunk 20 completada (16.89s). Nodos: 2, Rels: 1.\n",
      "2025-04-27 16:31:26,594 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 21 (len=168 chars)...\n",
      "2025-04-27 16:31:26,681 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "2025-04-27 16:31:41,564 - INFO - [__main__:186] - Extracción grafo chunk 21 completada (14.97s). Nodos: 2, Rels: 1.\n",
      "2025-04-27 16:31:41,564 - INFO - [__main__:173] - Iniciando extracción de grafo del chunk ID: 22 (len=142 chars)...\n",
      "2025-04-27 16:31:41,800 - INFO - [groq._base_client:1055] - Retrying request to /openai/v1/chat/completions in 15.000000 seconds\n",
      "2025-04-27 16:31:57,596 - INFO - [__main__:186] - Extracción grafo chunk 22 completada (16.03s). Nodos: 5, Rels: 4.\n",
      "2025-04-27 16:31:57,597 - INFO - [__main__:399] - Extracción de chunks para Microsoft.md completada. Exitosos/Intentados: 22/22, Fallos: 0.\n",
      "2025-04-27 16:31:57,597 - INFO - [__main__:415] - Agregando resultados de 22 partes extraídas para Microsoft.md...\n",
      "/tmp/ipykernel_73752/2274387750.py:281: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  \"nodes\": [node.dict() for node in aggregated_nodes.values()],\n",
      "/tmp/ipykernel_73752/2274387750.py:282: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  \"relationships\": [rel.dict() for rel in aggregated_relationships_list]\n",
      "2025-04-27 16:31:57,598 - INFO - [__main__:284] - Agregación completada. Nodos únicos: 55, Relaciones únicas: 44.\n",
      "2025-04-27 16:31:57,599 - INFO - [__main__:290] - Intentando guardar JSON en: output_graphs/Microsoft.graph.json\n",
      "2025-04-27 16:31:57,601 - INFO - [__main__:296] - Estructura del grafo guardada exitosamente.\n",
      "2025-04-27 16:31:57,601 - INFO - [__main__:427] - --- Procesamiento JSON para archivo Microsoft.md completado ---\n",
      "2025-04-27 16:31:57,602 - INFO - [__main__:79] - Extracción JSON completada para 2 archivos en 2567.09s.\n",
      "2025-04-27 16:31:57,602 - INFO - [__main__:112] - Bloque 9: Fase de extracción JSON completada (o saltada).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Visualización Previa JSON Extraído ---\n",
      "{\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"id\": \"InteligenciaArtificial\",\n",
      "      \"label\": \"Concept\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"RevolucionIndustrial\",\n",
      "      \"label\": \"Event\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"Internet\",\n",
      "      \"label\": \"Concept\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"Transformación\",\n",
      "      \"label\": \"Concept\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"Negocios\",\n",
      "      \"label\": \"Concept\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"Trabajo del conocimiento\",\n",
      "      \"label\": \"Concept\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"2025 Work Trend Index Annual Report\",\n",
      "      \"label\": \"Report\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"Microsoft 365\",\n",
      "      \"label\": \"Product\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"LinkedIn\",\n",
      "      \"label\": \"Platform\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"Experts\",\n",
      "      \"label\": \"Group\",\n",
      "      \"properties\": {}\n",
      "    }\n",
      "  ],\n",
      "  \"relationships\": [\n",
      "    {\n",
      "      \"source\": \"InteligenciaArtificial\",\n",
      "      \"target\": \"RevolucionIndustrial\",\n",
      "      \"type\": \"COMPARED_TO\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"InteligenciaArtificial\",\n",
      "      \"target\": \"Internet\",\n",
      "      \"type\": \"COMPARED_TO\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"2025 Work Trend Index Annual Report\",\n",
      "      \"target\": \"Microsoft 365\",\n",
      "      \"type\": \"BASED_ON\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"2025 Work Trend Index Annual Report\",\n",
      "      \"target\": \"LinkedIn\",\n",
      "      \"type\": \"BASED_ON\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"2025 Work Trend Index Annual Report\",\n",
      "      \"target\": \"Experts\",\n",
      "      \"type\": \"CONSULTED\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Year\",\n",
      "      \"target\": \"Leaders\",\n",
      "      \"type\": \"RELATED_TO\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Intelligence\",\n",
      "      \"target\": \"Tool\",\n",
      "      \"type\": \"RELATED_TO\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"Tool\",\n",
      "      \"target\": \"Work\",\n",
      "      \"type\": \"PART_OF\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"InteligenciaArtificial\",\n",
      "      \"target\": \"TrabajoDigital\",\n",
      "      \"type\": \"RELATED_TO\",\n",
      "      \"properties\": {}\n",
      "    },\n",
      "    {\n",
      "      \"source\": \"ProductivityPressure\",\n",
      "      \"target\": \"Leaders\",\n",
      "      \"type\": \"AFFECTS\",\n",
      "      \"properties\": {\n",
      "        \"percentage\": \"53\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "  (... 45 nodos más ...)\n",
      "  (... 34 relaciones más ...)\n",
      "--- (Mostrando hasta 10 nodos/relaciones. Total: 55 N, 44 R) ---\n",
      "-> Procesado (1257.03s). Nodos: 55, Rels: 44. JSON guardado.\n",
      "\n",
      "--- Resumen Extracción JSON (2567.09s) ---\n",
      "Archivos intentados: 2\n",
      "Procesados exitosamente (JSON generado): 2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# %% Bloque 9: Ejecución - Extracción de Grafo a JSON\n",
    "# ============================================================\n",
    "logger.info(\"Bloque 9: Iniciando fase de extracción de grafo a JSON...\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n=== Bloque 9: Extracción de Grafo a JSON ===\\n\" + \"=\"*50)\n",
    "\n",
    "processor: Optional[GraphProcessor] = None\n",
    "all_extracted_data: Dict[str, Optional[Dict[str, Any]]] = {} # Guarda datos JSON por archivo de entrada\n",
    "\n",
    "if not skip_extraction:\n",
    "    # Verificar dependencias clave\n",
    "    if not llm:\n",
    "        logger.error(\"LLM principal no inicializado. Saltando extracción.\")\n",
    "        print(\"ERROR: LLM principal no disponible. No se puede extraer.\")\n",
    "    elif not prompt_chunking or not prompt_extraction:\n",
    "         logger.error(\"Faltan plantillas de prompt. Saltando extracción.\")\n",
    "         print(\"ERROR: Faltan prompts necesarios. No se puede extraer.\")\n",
    "    else:\n",
    "        try:\n",
    "            logger.info(\"Instanciando GraphProcessor...\")\n",
    "            # Pasar `graph` (puede ser None si no se carga/consulta Neo4j)\n",
    "            # El driver directo no es necesario para la extracción JSON\n",
    "            processor = GraphProcessor(\n",
    "                main_llm=llm,\n",
    "                graph_instance=graph, # Pasar instancia Neo4jGraph (o None, solo usado para consultas opcionales)\n",
    "                chunking_prompt=prompt_chunking,\n",
    "                extraction_prompt=prompt_extraction,\n",
    "                ollama_chunking_model=ollama_chunking_model_name,\n",
    "                ollama_url=OLLAMA_BASE_URL\n",
    "                # schemas ya definidos por defecto\n",
    "            )\n",
    "            logger.info(\"GraphProcessor instanciado.\")\n",
    "            print(\"GraphProcessor listo para extracción.\")\n",
    "\n",
    "            # Procesar cada archivo\n",
    "            start_total_time = time.time()\n",
    "            num_files = len(input_filepaths)\n",
    "            processed_files_count = 0\n",
    "            failed_files_list = []\n",
    "\n",
    "            for i, filepath in enumerate(input_filepaths):\n",
    "                file_basename = os.path.basename(filepath)\n",
    "                logger.info(f\"--- Procesando archivo {i+1}/{num_files}: {file_basename} ---\")\n",
    "                print(f\"\\nProcesando archivo {i+1}/{num_files}: {file_basename}\")\n",
    "                start_file_time = time.time()\n",
    "                try:\n",
    "                    extracted_data = processor.process_file_to_json(\n",
    "                        filepath=filepath,\n",
    "                        output_dir=output_directory,\n",
    "                        print_chunks_flag=print_chunks,\n",
    "                        visualize_json_flag=visualize_json\n",
    "                    )\n",
    "                    all_extracted_data[filepath] = extracted_data # Guardar resultado (incluso None o vacío)\n",
    "                    file_duration = time.time() - start_file_time\n",
    "                    if extracted_data is not None: # Considerar éxito si devuelve algo\n",
    "                        processed_files_count += 1\n",
    "                        # Contar nodos/rels extraídos si no es None\n",
    "                        node_count = len(extracted_data.get(\"nodes\", [])) if extracted_data else 0\n",
    "                        rel_count = len(extracted_data.get(\"relationships\", [])) if extracted_data else 0\n",
    "                        print(f\"-> Procesado ({file_duration:.2f}s). Nodos: {node_count}, Rels: {rel_count}. JSON guardado.\")\n",
    "                    else:\n",
    "                        failed_files_list.append(file_basename)\n",
    "                        print(f\"-> Fallo durante el procesamiento ({file_duration:.2f}s) (ver logs).\")\n",
    "                except KeyboardInterrupt:\n",
    "                     logger.warning(\"Procesamiento interrumpido por el usuario.\")\n",
    "                     print(\"\\nInterrupción por teclado detectada. Abortando...\")\n",
    "                     raise # Relanzar para detener completamente\n",
    "                except Exception as file_e: # Capturar errores inesperados por archivo\n",
    "                     file_duration = time.time() - start_file_time\n",
    "                     logger.error(f\"Error fatal procesando archivo {filepath} ({file_duration:.2f}s): {file_e}\", exc_info=True)\n",
    "                     print(f\"-> ERROR FATAL procesando {file_basename} ({file_duration:.2f}s) (ver logs).\")\n",
    "                     failed_files_list.append(file_basename)\n",
    "                     all_extracted_data[filepath] = None # Marcar como fallido\n",
    "                # Pausa opcional entre archivos para evitar rate limits o sobrecarga\n",
    "                # time.sleep(1)\n",
    "\n",
    "            end_total_time = time.time()\n",
    "            total_duration_str = f\"{end_total_time - start_total_time:.2f}s\"\n",
    "            logger.info(f\"Extracción JSON completada para {num_files} archivos en {total_duration_str}.\")\n",
    "            print(f\"\\n--- Resumen Extracción JSON ({total_duration_str}) ---\")\n",
    "            print(f\"Archivos intentados: {num_files}\")\n",
    "            print(f\"Procesados exitosamente (JSON generado): {processed_files_count}\")\n",
    "            if failed_files_list:\n",
    "                print(f\"Archivos con fallos durante procesamiento: {len(failed_files_list)} ({', '.join(failed_files_list)})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fatal durante instanciación o bucle de extracción: {e}\", exc_info=True)\n",
    "            print(f\"¡ERROR FATAL durante el proceso de extracción!: {e}\")\n",
    "            processor = None # Asegurar que no se use si falló\n",
    "else:\n",
    "    logger.info(\"'skip_extraction' activado. Saltando extracción JSON.\")\n",
    "    print(\"Extracción JSON omitida.\")\n",
    "    # Instanciar procesador si se necesita para consultas Neo4j posteriores\n",
    "    if run_interactive_query and load_into_neo4j:\n",
    "        # Solo instanciar si los componentes necesarios están listos\n",
    "        if llm and graph and prompt_chunking and prompt_extraction:\n",
    "             try:\n",
    "                 logger.info(\"Instanciando GraphProcessor (skip extraction) para consultas Neo4j...\")\n",
    "                 processor = GraphProcessor(\n",
    "                     main_llm=llm, graph_instance=graph, chunking_prompt=prompt_chunking,\n",
    "                     extraction_prompt=prompt_extraction, ollama_chunking_model=ollama_chunking_model_name,\n",
    "                     ollama_url=OLLAMA_BASE_URL\n",
    "                 )\n",
    "                 print(\"GraphProcessor instanciado (extracción omitida) para consultas.\")\n",
    "             except Exception as e:\n",
    "                 logger.error(f\"Fallo instanciación tardía: {e}\"); processor=None\n",
    "        else:\n",
    "             logger.warning(\"Faltan componentes para instanciar GraphProcessor para consultas.\")\n",
    "             print(\"Advertencia: No se pudo preparar para consultas (faltan llm/graph/prompts).\")\n",
    "\n",
    "\n",
    "logger.info(\"Bloque 9: Fase de extracción JSON completada (o saltada).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f04c5f4-e0fe-4bfe-a0b3-dabb28264f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 16:31:57,612 - INFO - [__main__:4] - Bloque 10: Iniciando carga opcional a Neo4j...\n",
      "2025-04-27 16:31:57,613 - INFO - [__main__:8] - 'load_into_neo4j' activado.\n",
      "2025-04-27 16:31:57,614 - INFO - [__main__:27] - Cargando datos JSON del archivo: Conciencia.md a Neo4j...\n",
      "2025-04-27 16:31:57,614 - INFO - [__main__:445] - Iniciando carga a Neo4j: 33 nodos, 40 relaciones...\n",
      "2025-04-27 16:31:57,626 - INFO - [__main__:460] - APOC detectado. Se usarán funciones APOC para carga.\n",
      "2025-04-27 16:31:57,689 - INFO - [__main__:501] - Lotes de nodos procesados/intentados en Neo4j: 33\n",
      "2025-04-27 16:31:57,763 - INFO - [__main__:543] - Lotes de relaciones procesados/intentados en Neo4j: 40\n",
      "2025-04-27 16:31:57,764 - INFO - [__main__:555] - Carga a Neo4j (transacción) finalizada en 0.15s. Errores DENTRO de la transacción: 0\n",
      "2025-04-27 16:31:57,764 - INFO - [__main__:27] - Cargando datos JSON del archivo: Microsoft.md a Neo4j...\n",
      "2025-04-27 16:31:57,765 - INFO - [__main__:445] - Iniciando carga a Neo4j: 55 nodos, 44 relaciones...\n",
      "2025-04-27 16:31:57,767 - INFO - [__main__:460] - APOC detectado. Se usarán funciones APOC para carga.\n",
      "2025-04-27 16:31:57,782 - INFO - [__main__:501] - Lotes de nodos procesados/intentados en Neo4j: 55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "=== Bloque 10: Carga Opcional a Neo4j ===\n",
      "==================================================\n",
      "Intentando cargar datos JSON extraídos a Neo4j...\n",
      "Cargando: Conciencia.md...\n",
      "  -> Carga exitosa.\n",
      "Cargando: Microsoft.md...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 16:31:57,811 - INFO - [__main__:543] - Lotes de relaciones procesados/intentados en Neo4j: 44\n",
      "2025-04-27 16:31:57,812 - INFO - [__main__:555] - Carga a Neo4j (transacción) finalizada en 0.05s. Errores DENTRO de la transacción: 0\n",
      "2025-04-27 16:31:57,813 - INFO - [__main__:65] - Resumen carga Neo4j: Intentados=2, Éxitos=2, Errores=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Carga exitosa.\n",
      "\n",
      "--- Resumen Carga Neo4j (0.20s) ---\n",
      "Archivos con datos para intentar cargar: 2\n",
      "Cargados exitosamente (transacciones completadas): 2\n",
      "\n",
      "--- Estado del Grafo Neo4j Post-Carga ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 16:31:57,836 - INFO - [__main__:145] - Consultando índices en BD 'default'...\n",
      "2025-04-27 16:31:57,859 - INFO - [__main__:166] - Se encontraron 2 índices en BD 'default'.\n",
      "2025-04-27 16:31:57,859 - INFO - [__main__:78] - 'show_neo4j_browser' activado post-carga.\n",
      "2025-04-27 16:31:57,860 - WARNING - [__main__:301] - webbrowser.open devolvió False para http://tika_neo4j_02:7474/browser/. Puede que no se haya abierto.\n",
      "2025-04-27 16:31:57,860 - INFO - [__main__:98] - Bloque 10: Fase de carga opcional a Neo4j completada.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Nodos Totales (APOC): 86\n",
      "- Relaciones Totales (APOC): 84\n",
      "- Tipos de Nodos (Labels): Concept, Date, Event, Group, Organization, Person, Platform, Product, Program, Report, Year\n",
      "- Tipos de Relaciones: ()-[:AFFECTS]->(), ()-[:AFFECTS]->(:Group), ()-[:ANALOGY]->(), ()-[:ANALOGY]->(:Group), ()-[:APPLIES_TO]->(), ()-[:APPLIES_TO]->(:Concept), ()-[:APPLIES_TO]->(:Group), ()-[:ARE_LESS_FAMILIAR_WITH]->(), ()-[:ARE_LESS_FAMILIAR_WITH]->(:Concept), ()-[:ARE_MORE_FAMILIAR_WITH]->(), ()-[:ARE_MORE_FAMILIAR_WITH]->(:Concept), ()-[:AUTOMATES]->(), ()-[:AUTOMATES]->(:Concept), ()-[:BASED_ON]->(), ()-[:BASED_ON]->(:Concept), ()-[:BASED_ON]->(:Platform), ()-[:BASED_ON]->(:Product), ()-[:BELIEVE]->(), ()-[:BELIEVE]->(:Concept), ()-[:COMPARED_TO]->(), ()-[:COMPARED_TO]->(:Concept), ()-[:COMPARED_TO]->(:Event), ()-[:CONSULTED]->(), ()-[:CONSULTED]->(:Group), ()-[:DEVELOPED_BY]->(), ()-[:DEVELOPED_BY]->(:Organization), ()-[:DIFFERS_FROM]->(), ()-[:DIFFERS_FROM]->(:Concept), ()-[:DISCUSSES]->(), ()-[:DISCUSSES]->(:Concept), ()-[:EVALUATES]->(), ()-[:EVALUATES]->(:Concept), ()-[:EXHIBITS]->(), ()-[:EXHIBITS]->(:Concept), ()-[:EXPECT_TO_EXPAND]->(), ()-[:EXPECT_TO_EXPAND]->(:Date), ()-[:EXPECT_TO_USE]->(), ()-[:EXPECT_TO_USE]->(:Concept), ()-[:HAS_CONFIDENCE]->(), ()-[:HAS_CONFIDENCE]->(:Group), ()-[:HAS_FEATURE]->(), ()-[:HAS_FEATURE]->(:Concept), ()-[:IMITATES]->(), ()-[:IMITATES]->(:Person), ()-[:INSTANCE_OF]->(), ()-[:INSTANCE_OF]->(:Concept), ()-[:INTEGRATES]->(), ()-[:INTEGRATES]->(:Concept), ()-[:LACKS]->(), ()-[:LACKS]->(:Concept), ()-[:MENTIONS]->(), ()-[:MENTIONS]->(:Concept), ()-[:MENTIONS]->(:Platform), ()-[:PART_OF]->(), ()-[:PART_OF]->(:Concept), ()-[:PART_OF]->(:Program), ()-[:RELATED_TO]->(), ()-[:RELATED_TO]->(:Concept), ()-[:RELATED_TO]->(:Group), ()-[:RELATED_TO]->(:Organization), ()-[:RELATED_TO]->(:Person), ()-[:RELATED_TO]->(:Product), ()-[:REPORTS]->(), ()-[:REPORTS]->(:Concept), ()-[:STUDIES]->(), ()-[:STUDIES]->(:Person), ()-[:SUBCLASS_OF]->(), ()-[:SUBCLASS_OF]->(:Concept), ()-[:USES]->(), ()-[:USES]->(:Concept), ()-[:USES]->(:Organization), ()-[:WORKS_AT]->(), ()-[:WORKS_AT]->(:Organization), ()-[:WORKS_AT]->(:Person), ()-[:WORKS_WITH]->(), ()-[:WORKS_WITH]->(:Concept), ()-[:WORKS_WITH]->(:Person), (:Concept)-[:AFFECTS]->(), (:Concept)-[:APPLIES_TO]->(), (:Concept)-[:AUTOMATES]->(), (:Concept)-[:COMPARED_TO]->(), (:Concept)-[:EVALUATES]->(), (:Concept)-[:HAS_CONFIDENCE]->(), (:Concept)-[:HAS_FEATURE]->(), (:Concept)-[:INSTANCE_OF]->(), (:Concept)-[:INTEGRATES]->(), (:Concept)-[:MENTIONS]->(), (:Concept)-[:PART_OF]->(), (:Concept)-[:RELATED_TO]->(), (:Concept)-[:REPORTS]->(), (:Concept)-[:STUDIES]->(), (:Group)-[:ANALOGY]->(), (:Group)-[:ARE_MORE_FAMILIAR_WITH]->(), (:Group)-[:BASED_ON]->(), (:Group)-[:BELIEVE]->(), (:Group)-[:DIFFERS_FROM]->(), (:Group)-[:DISCUSSES]->(), (:Group)-[:EXHIBITS]->(), (:Group)-[:EXPECT_TO_EXPAND]->(), (:Group)-[:EXPECT_TO_USE]->(), (:Group)-[:IMITATES]->(), (:Group)-[:LACKS]->(), (:Group)-[:PART_OF]->(), (:Group)-[:RELATED_TO]->(), (:Group)-[:SUBCLASS_OF]->(), (:Group)-[:USES]->(), (:Group)-[:WORKS_AT]->(), (:Organization)-[:COMPARED_TO]->(), (:Organization)-[:RELATED_TO]->(), (:Organization)-[:WORKS_AT]->(), (:Organization)-[:WORKS_WITH]->(), (:Person)-[:ARE_LESS_FAMILIAR_WITH]->(), (:Person)-[:BELIEVE]->(), (:Person)-[:MENTIONS]->(), (:Person)-[:REPORTS]->(), (:Product)-[:DEVELOPED_BY]->(), (:Product)-[:PART_OF]->(), (:Product)-[:USES]->(), (:Report)-[:BASED_ON]->(), (:Report)-[:CONSULTED]->(), (:Year)-[:RELATED_TO]->()\n",
      "\n",
      "--- Índices en Base de Datos: default ---\n",
      "Se encontraron 2 índices:\n",
      "- Nombre: index_343aff4e | Estado: ONLINE | TipoÍndice: LOOKUP\n",
      "    TipoEntidad: NODE | Labels/Tipos: N/A | Propiedades: N/A\n",
      "- Nombre: index_f7700477 | Estado: ONLINE | TipoÍndice: LOOKUP\n",
      "    TipoEntidad: RELATIONSHIP | Labels/Tipos: N/A | Propiedades: N/A\n",
      "---------------------------------------\n",
      "--------------------------------------------------\n",
      "\n",
      "Intentando abrir Neo4j Browser...\n",
      "No se pudo confirmar apertura automática del navegador en http://tika_neo4j_02:7474/browser/. Abre la URL manualmente si es necesario.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# %% Bloque 10: Ejecución - Carga Opcional a Neo4j\n",
    "# ============================================================\n",
    "logger.info(\"Bloque 10: Iniciando carga opcional a Neo4j...\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n=== Bloque 10: Carga Opcional a Neo4j ===\\n\" + \"=\"*50)\n",
    "\n",
    "if load_into_neo4j:\n",
    "    logger.info(\"'load_into_neo4j' activado.\")\n",
    "    # Necesitamos el driver directo para la carga robusta con transacciones\n",
    "    if driver: # Verificar si el driver está disponible (creado en Bloque 8)\n",
    "        print(\"Intentando cargar datos JSON extraídos a Neo4j...\")\n",
    "        total_files_attempted_load = 0\n",
    "        files_loaded_successfully = 0\n",
    "        files_with_load_errors = []\n",
    "        start_load_time = time.time()\n",
    "\n",
    "        # Iterar sobre los datos extraídos en el bloque anterior\n",
    "        if 'all_extracted_data' not in locals():\n",
    "             logger.warning(\"Variable 'all_extracted_data' no encontrada. ¿Se saltó la extracción?\")\n",
    "             all_extracted_data = {} # Evitar error\n",
    "\n",
    "        for filepath, graph_data in all_extracted_data.items():\n",
    "            file_basename = os.path.basename(filepath)\n",
    "            # Cargar solo si hay datos válidos extraídos\n",
    "            if graph_data is not None and isinstance(graph_data, dict) and (graph_data.get(\"nodes\") or graph_data.get(\"relationships\")):\n",
    "                total_files_attempted_load += 1\n",
    "                logger.info(f\"Cargando datos JSON del archivo: {file_basename} a Neo4j...\")\n",
    "                print(f\"Cargando: {file_basename}...\")\n",
    "                try:\n",
    "                    # Asegurar que processor esté disponible si es necesario (si el método de carga NO fuera estático)\n",
    "                    # if 'processor' not in locals() or not processor:\n",
    "                    #     if llm and prompt_chunking and prompt_extraction:\n",
    "                    #         logger.warning(\"Instanciando GraphProcessor mínimamente...\")\n",
    "                    #         processor = GraphProcessor(llm, graph, prompt_chunking, prompt_extraction, ollama_chunking_model_name, OLLAMA_BASE_URL)\n",
    "                    #     else:\n",
    "                    #         raise RuntimeError(\"GraphProcessor no instanciado y faltan componentes.\")\n",
    "\n",
    "                    # Llamar al método de carga (pasando el driver)\n",
    "                    # Asumiendo que _load_graph_data_to_neo4j está en la clase Processor\n",
    "                    # Si la hiciste independiente, llama a esa función directamente\n",
    "                    if not processor: raise RuntimeError(\"GraphProcessor no está instanciado para llamar a _load_graph_data_to_neo4j\")\n",
    "                    success = processor._load_graph_data_to_neo4j(graph_data, driver, NEO4J_DATABASE)\n",
    "\n",
    "                    if success:\n",
    "                        files_loaded_successfully += 1\n",
    "                        print(f\"  -> Carga exitosa.\")\n",
    "                    else:\n",
    "                        files_with_load_errors.append(file_basename)\n",
    "                        print(f\"  -> Error durante la carga (transacción fallida, ver logs).\")\n",
    "                except Exception as load_e:\n",
    "                     logger.error(f\"Error inesperado llamando a carga para {file_basename}: {load_e}\", exc_info=True)\n",
    "                     files_with_load_errors.append(file_basename)\n",
    "                     print(f\"  -> ERROR FATAL durante la carga (ver logs).\")\n",
    "            elif graph_data is None:\n",
    "                logger.warning(f\"Carga omitida para {file_basename} porque la extracción falló.\")\n",
    "            else: # Datos vacíos\n",
    "                logger.info(f\"Carga omitida para {file_basename} (sin nodos/rels extraídos).\")\n",
    "\n",
    "        load_duration = time.time() - start_load_time\n",
    "        print(f\"\\n--- Resumen Carga Neo4j ({load_duration:.2f}s) ---\")\n",
    "        print(f\"Archivos con datos para intentar cargar: {total_files_attempted_load}\")\n",
    "        print(f\"Cargados exitosamente (transacciones completadas): {files_loaded_successfully}\")\n",
    "        if files_with_load_errors:\n",
    "            print(f\"Archivos con errores durante carga (transacción fallida): {len(files_with_load_errors)} ({', '.join(files_with_load_errors)})\")\n",
    "        logger.info(f\"Resumen carga Neo4j: Intentados={total_files_attempted_load}, Éxitos={files_loaded_successfully}, Errores={len(files_with_load_errors)}\")\n",
    "\n",
    "        # Mostrar estado final y abrir navegador (solo si el driver sigue siendo válido)\n",
    "        if driver:\n",
    "            print(\"\\n--- Estado del Grafo Neo4j Post-Carga ---\")\n",
    "            if 'retrieve_graph_summary' not in locals() or 'print_indexes' not in locals():\n",
    "                print(\"Funciones de resumen/índices no definidas (Ejecuta Bloque 5).\")\n",
    "            else:\n",
    "                print(retrieve_graph_summary(driver, db_name=NEO4J_DATABASE))\n",
    "                print_indexes(driver, db_name=NEO4J_DATABASE)\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            if show_neo4j_browser:\n",
    "                logger.info(\"'show_neo4j_browser' activado post-carga.\")\n",
    "                print(\"\\nIntentando abrir Neo4j Browser...\")\n",
    "                neo4j_browser_url = \"http://localhost:7474/browser/\"\n",
    "                try: host = urlparse(NEO4J_URI).hostname\n",
    "                except Exception: host = None\n",
    "                if host and host not in [\"localhost\", \"127.0.0.1\"]: neo4j_browser_url = f\"http://{host}:7474/browser/\"\n",
    "                if 'display_neo4j_browser' in locals():\n",
    "                    display_neo4j_browser(neo4j_browser_url)\n",
    "                else: print(\"Función display_neo4j_browser no definida (Ejecuta Bloque 5).\")\n",
    "        else:\n",
    "             print(\"\\nNo se puede mostrar estado final de Neo4j (driver no disponible).\")\n",
    "\n",
    "    elif not driver:\n",
    "         logger.error(\"Carga a Neo4j omitida: Driver Neo4j no disponible o cerrado previamente.\")\n",
    "         print(\"ERROR: Carga a Neo4j omitida (Driver Neo4j no listo).\")\n",
    "\n",
    "else:\n",
    "    logger.info(\"'load_into_neo4j' desactivado. Saltando carga a Neo4j.\")\n",
    "    print(\"Carga a Neo4j desactivada.\")\n",
    "\n",
    "logger.info(\"Bloque 10: Fase de carga opcional a Neo4j completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d47ad-58a1-4b6d-96c1-c4d9f5a01a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# %% Bloque 11: Ejecución - Consulta Interactiva (Opcional, si Neo4j cargado)\n",
    "# ============================================================\n",
    "logger.info(\"Bloque 11: Iniciando consulta interactiva opcional...\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n=== Bloque 11: Consulta Interactiva Opcional (Neo4j) ===\\n\" + \"=\"*50)\n",
    "\n",
    "if run_interactive_query:\n",
    "    logger.info(\"'run_interactive_query' activado.\")\n",
    "    # Requiere que Neo4j se haya cargado (implícito si load_into_neo4j=True)\n",
    "    # Y que el procesador (con wrapper 'graph') esté listo E instanciado (puede no estarlo si skip_extraction=True)\n",
    "    if load_into_neo4j: # Solo permitir si se cargó\n",
    "        if processor and graph and prompt_query: # graph (wrapper) solo se crea si run_interactive_query=True en Bloque 8\n",
    "            logger.info(\"Iniciando sesión de consulta interactiva Neo4j...\")\n",
    "            try: processor.interactive_query(prompt_query)\n",
    "            except Exception as e: logger.error(f\"Error sesión interactiva: {e}\", exc_info=True)\n",
    "            logger.info(\"Sesión consulta interactiva Neo4j finalizada.\")\n",
    "        elif not processor: logger.error(\"Consulta omitida: GraphProcessor no instanciado.\"); print(\"Error consulta: GP no instanciado.\")\n",
    "        elif not graph: logger.error(\"Consulta omitida: Neo4j Graph Wrapper no inicializado (¿run_interactive_query era True en Bloque 8?).\"); print(\"Error consulta: Wrapper Neo4j no listo.\")\n",
    "        else: logger.error(\"Consulta omitida: Falta prompt de consulta.\"); print(\"Error consulta: Prompt falta.\")\n",
    "    else:\n",
    "         logger.info(\"Consulta interactiva omitida porque 'load_into_neo4j' es False.\")\n",
    "         print(\"Consulta interactiva omitida (datos no cargados a Neo4j).\")\n",
    "else:\n",
    "    logger.info(\"'run_interactive_query' desactivado.\")\n",
    "    print(\"Modo de consulta interactiva desactivado.\")\n",
    "\n",
    "logger.info(\"Bloque 11: Fase de consulta interactiva completada (o saltada).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec6483-0d6c-4ec9-aeb1-9dcecb890412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# %% Bloque 12: Limpieza Final\n",
    "# ============================================================\n",
    "logger.info(\"Bloque 12: Iniciando limpieza final...\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n=== Bloque 12: Limpieza Final ===\\n\" + \"=\"*50)\n",
    "\n",
    "# Cerrar driver directo Neo4j si se abrió\n",
    "if 'driver' in locals() and driver and driver.is_open():\n",
    "    try:\n",
    "        driver.close(); logger.info(\"Conexión driver Neo4j cerrada.\"); print(\"Conexión directa Neo4j cerrada.\")\n",
    "    except Exception as e: logger.error(f\"Error cerrando driver Neo4j: {e}\"); print(f\"Error al cerrar conexión Neo4j: {e}\")\n",
    "else:\n",
    "    logger.info(\"Driver Neo4j ya cerrado o no inicializado.\")\n",
    "\n",
    "# El objeto `graph` (Neo4jGraph wrapper) maneja su pool internamente.\n",
    "\n",
    "logger.info(\"Bloque 12: Limpieza final completada.\")\n",
    "print(\"\\nEjecución del script completada.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a210943-8ce3-43d3-a005-15d65f629562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 13:30:46,350 - WARNING - [__main__:8] - Bloque OPCIONAL de Limpieza Total Neo4j iniciado...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "=== Bloque OPCIONAL: Limpieza Completa Neo4j ===\n",
      "==================================================\n",
      "¡¡¡ADVERTENCIA MUY SERIA!!!\n",
      "\n",
      "Se intentará conectar a: bolt://TIKA_Neo4j_02:7687 (Usuario: neo4j, BD: neo4j)\n",
      "Para BORRAR **TODO** su contenido.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Para proceder con el borrado IRREVERSIBLE de la BD 'neo4j', escribe 'SI QUIERO BORRAR TODO':  SI QUIERO BORRAR TODO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 13:31:09,771 - WARNING - [__main__:29] - Confirmación recibida. Intentando borrar TODO en BD 'neo4j'...\n",
      "2025-04-27 13:31:09,774 - INFO - [__main__:31] - Obteniendo driver temporal para limpieza...\n",
      "2025-04-27 13:31:09,776 - ERROR - [__main__:37] - La función 'get_neo4j_driver' no está definida (ejecuta Bloque 5).\n",
      "2025-04-27 13:31:09,778 - ERROR - [__main__:59] - No se pudo obtener el driver de Neo4j para la limpieza.\n",
      "2025-04-27 13:31:09,781 - INFO - [__main__:77] - Bloque OPCIONAL de Limpieza Total Neo4j finalizado.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Función 'get_neo4j_driver' no encontrada.\n",
      "ERROR: No se pudo conectar a Neo4j para limpiar.\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# %% Bloque OPCIONAL: Limpieza Completa de la Base de Datos Neo4j\n",
    "# ============================================================\n",
    "# ADVERTENCIA: ¡Este bloque BORRARÁ TODOS los nodos y relaciones\n",
    "# de la base de datos Neo4j especificada en el Bloque 2!\n",
    "# Ejecuta esta celda SOLO si estás SEGURO de querer limpiar la BD.\n",
    "\n",
    "logger.warning(\"Bloque OPCIONAL de Limpieza Total Neo4j iniciado...\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n=== Bloque OPCIONAL: Limpieza Completa Neo4j ===\\n\" + \"=\"*50)\n",
    "print(\"¡¡¡ADVERTENCIA MUY SERIA!!!\")\n",
    "\n",
    "# Verificar si las variables de conexión están definidas (del Bloque 2)\n",
    "if 'NEO4J_URI' not in locals() or 'NEO4J_USERNAME' not in locals() or 'NEO4J_PASSWORD' not in locals():\n",
    "    logger.error(\"Variables de conexión Neo4j (NEO4J_URI, _USERNAME, _PASSWORD) no definidas. Ejecuta el Bloque 2 primero.\")\n",
    "    print(\"ERROR: Faltan variables de conexión Neo4j. Ejecuta el Bloque 2.\")\n",
    "elif not NEO4J_PASSWORD:\n",
    "     logger.error(\"Falta NEO4J_PASSWORD. No se puede conectar para limpiar.\")\n",
    "     print(\"ERROR: Falta la contraseña de Neo4j (NEO4J_PASSWORD).\")\n",
    "else:\n",
    "    # Conectarse usando el driver directo\n",
    "    temp_driver: Optional[Driver] = None\n",
    "    effective_db_name = NEO4J_DATABASE if NEO4J_DATABASE else \"neo4j\" # Nombre para mostrar\n",
    "    print(f\"\\nSe intentará conectar a: {NEO4J_URI} (Usuario: {NEO4J_USERNAME}, BD: {effective_db_name})\")\n",
    "    print(f\"Para BORRAR **TODO** su contenido.\")\n",
    "\n",
    "    confirm = input(f\"\\nPara proceder con el borrado IRREVERSIBLE de la BD '{effective_db_name}', escribe 'SI QUIERO BORRAR TODO': \")\n",
    "\n",
    "    if confirm == \"SI QUIERO BORRAR TODO\":\n",
    "        logger.warning(f\"Confirmación recibida. Intentando borrar TODO en BD '{effective_db_name}'...\")\n",
    "        try:\n",
    "            logger.info(\"Obteniendo driver temporal para limpieza...\")\n",
    "            # Usar la función get_neo4j_driver definida en Bloque 5\n",
    "            # Asegúrate que el Bloque 5 se haya ejecutado o copia la función aquí\n",
    "            if 'get_neo4j_driver' in locals():\n",
    "                 temp_driver = get_neo4j_driver(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "            else:\n",
    "                 logger.error(\"La función 'get_neo4j_driver' no está definida (ejecuta Bloque 5).\")\n",
    "                 print(\"ERROR: Función 'get_neo4j_driver' no encontrada.\")\n",
    "                 # Podrías copiar la definición aquí como fallback si es necesario\n",
    "\n",
    "            if temp_driver:\n",
    "                logger.info(f\"Conectado a Neo4j. Procediendo con borrado en BD '{effective_db_name}'...\")\n",
    "                # Usar la función reset_graph_data definida en Bloque 5\n",
    "                # Esta función ya incluye una confirmación interna adicional\n",
    "                if 'reset_graph_data' in locals():\n",
    "                    # La función reset_graph_data pide una SEGUNDA confirmación\n",
    "                    reset_success = reset_graph_data(temp_driver, db_name=NEO4J_DATABASE)\n",
    "                    if reset_success:\n",
    "                        print(f\"\\n¡BORRADO COMPLETO de BD '{effective_db_name}' realizado!\")\n",
    "                        logger.info(f\"Limpieza completa de BD '{effective_db_name}' finalizada exitosamente.\")\n",
    "                    else:\n",
    "                        print(f\"\\nBorrado de BD '{effective_db_name}' falló o fue cancelado en la segunda confirmación.\")\n",
    "                        logger.warning(f\"Borrado de BD '{effective_db_name}' falló o fue cancelado.\")\n",
    "                else:\n",
    "                     logger.error(\"La función 'reset_graph_data' no está definida (ejecuta Bloque 5).\")\n",
    "                     print(\"ERROR: Función 'reset_graph_data' no encontrada.\")\n",
    "\n",
    "            else:\n",
    "                logger.error(\"No se pudo obtener el driver de Neo4j para la limpieza.\")\n",
    "                print(\"ERROR: No se pudo conectar a Neo4j para limpiar.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error inesperado durante el proceso de limpieza: {e}\", exc_info=True)\n",
    "            print(f\"ERROR inesperado durante la limpieza: {e}\")\n",
    "        finally:\n",
    "            # Asegurarse de cerrar el driver temporal\n",
    "            if temp_driver and temp_driver.is_open():\n",
    "                try:\n",
    "                    temp_driver.close()\n",
    "                    logger.info(\"Driver temporal de limpieza cerrado.\")\n",
    "                except Exception as close_e:\n",
    "                    logger.error(f\"Error cerrando driver temporal: {close_e}\")\n",
    "    else:\n",
    "        logger.info(\"Confirmación para borrado completo NO recibida. Operación cancelada.\")\n",
    "        print(\"\\nBorrado completo CANCELADO.\")\n",
    "\n",
    "logger.info(\"Bloque OPCIONAL de Limpieza Total Neo4j finalizado.\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c979f1c-d3e6-48f6-ba8d-1313f76546ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
